<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="en">
	<title>Modern Tragedy</title>
	<subtitle>programming stuff</subtitle>
	<link href="https://hhow09.github.io/feed/feed.xml" rel="self"/>
	<link href="https://hhow09.github.io/"/>
	<updated>2023-11-12T00:00:00Z</updated>
	<id>https://hhow09.github.io/</id>
	<author>
		<name>HH</name>
		<email>hhow09@gmail.com</email>
	</author>
	
	<entry>
		<title>Character Sets and Encodings</title>
		<link href="https://hhow09.github.io/blog/charset-encoding/"/>
		<updated>2023-11-12T00:00:00Z</updated>
		<id>https://hhow09.github.io/blog/charset-encoding/</id>
		<content type="html">&lt;h2 id=&quot;ascii&quot; tabindex=&quot;-1&quot;&gt;ASCII &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc20&quot;&gt;RFC 20 ASCII format for Network Interchange&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ASCII is a character set
&lt;ul&gt;
&lt;li&gt;Codes 0-31 are unprintable control codes and are used to control peripherals such as printers.&lt;/li&gt;
&lt;li&gt;Codes 32-127 are called printable characters, they are for all the &lt;a href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;different variations of the ASCII table&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use 8 bits to represent individual characters. (7-bit in early age)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc2616&quot;&gt;HTTP 1.1&lt;/a&gt; uses &lt;code&gt;US-ASCII&lt;/code&gt; as basic character set for the &lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc2616#section-4.2&quot;&gt;request line&lt;/a&gt; in requests, the &lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc2616#section-6.1.1&quot;&gt;status line&lt;/a&gt; in responses (except the &lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc2616#section-6.1.1&quot;&gt;reason phrase&lt;/a&gt;) and the &lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc2616#section-4.2&quot;&gt;field names&lt;/a&gt; but allows any octet in the field values and the &lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc2616#section-4.3&quot;&gt;message body&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;eascii&quot; tabindex=&quot;-1&quot;&gt;EASCII &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;extended ASCII codes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;unicode&quot; tabindex=&quot;-1&quot;&gt;UNICODE &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.rfc-editor.org/rfc/rfc5198&quot;&gt;RFC 5198: Unicode Format for Network Interchange&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://home.unicode.org/&quot;&gt;unicode org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;The Unicode Standard refers to the standard character set that represents all natural language characters. Unicode can encode up to roughly 1.1 million characters, allowing it to support all of the world’s languages and scripts in a single, universal standard.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;UNICODE is &lt;code&gt;ASCII&lt;/code&gt; compatible (&lt;code&gt;U+0000&lt;/code&gt; to &lt;code&gt;U+007F&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;utf-8&quot; tabindex=&quot;-1&quot;&gt;UTF-8 &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc3629&quot;&gt;RFC 3629: UTF-8, a transformation format of ISO 10646&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UTF-8 is defined by the Unicode Standard [&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc3629#ref-UNICODE&quot;&gt;UNICODE&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;In UTF-8, characters from the &lt;code&gt;U+0000..U+10FFFF&lt;/code&gt; range  are encoded using sequences of 1 to 4 octets.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;   Char. number range  |        UTF-8 octet sequence
      (hexadecimal)    |              (binary)
   --------------------+------------------------------------
   0000 0000-0000 007F | 0xxxxxxx
   0000 0080-0000 07FF | 110xxxxx 10xxxxxx
   0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx
   0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;example&quot; tabindex=&quot;-1&quot;&gt;Example &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HTML header: &lt;code&gt;Content-Type: text/plain; charset=&amp;quot;UTF-8&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Golang: &lt;a href=&quot;https://go.dev/ref/spec#Source_code_representation&quot;&gt;Rune literals&lt;/a&gt; use UTF-8&lt;/li&gt;
&lt;li&gt;Rust: &lt;a href=&quot;https://doc.rust-lang.org/std/string/struct.String.html&quot;&gt;Struct std::string::String&lt;/a&gt; A UTF-8–encoded, growable string.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;base64&quot; tabindex=&quot;-1&quot;&gt;Base64 &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;RFC: &lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc4648#section-4&quot;&gt;rfc4648#section-4&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;24 bits byte sequence can be represented by four &lt;strong&gt;6-bit Base64 digits&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 chars are used to represent &lt;code&gt;4 * 6 = 24 bits = 3 bytes&lt;/code&gt; (if we ignore the &lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc4648#section-3.2&quot;&gt;padding&lt;/a&gt; and round-up detail)&lt;/li&gt;
&lt;li&gt;3-char string will become 4-char string after the encoding, which the means size will increase by about 33%.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Used when there was a need to encode &lt;strong&gt;binary data&lt;/strong&gt; so that it can be stored and transferred over mediums that primarily designed to deal with ASCII text. E-Mail attachments are sent out as base64 encoded strings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;IS: case sensitive&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In Unix system, &lt;a href=&quot;https://en.wikipedia.org/wiki/Crypt_(C)&quot;&gt;crypt()&lt;/a&gt; uses a special Base64-type of encoding. It uses &lt;code&gt;./0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&lt;/code&gt; to encode the hashed password.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;example-1&quot; tabindex=&quot;-1&quot;&gt;Example: &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In binary, &amp;quot;cat&amp;quot; is &lt;code&gt;01100011 01100001 01110100&lt;/code&gt; ( 3 bytes)&lt;/li&gt;
&lt;li&gt;base 64 &amp;quot;cat&amp;quot; would be&lt;pre&gt;&lt;code&gt;011000 110110 000101 110100
|      |      |      |
Y      2      F      0
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;example-generate-random-password&quot; tabindex=&quot;-1&quot;&gt;Example - Generate random password &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Language-specific characters are typically avoided by password generators because they would not be universally available (US keyboards don&#39;t have accented characters, for instance). So don&#39;t take their omission from these tools as an indication that they might be weak or problematic.
- &lt;a href=&quot;https://security.stackexchange.com/questions/225346/is-it-bad-to-use-special-characters-in-passwords&quot;&gt;Is it bad to use special characters in passwords? [duplicate]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&quot;language-go&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-go&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;package&lt;/span&gt; main
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
	&lt;span class=&quot;token string&quot;&gt;&quot;crypto/rand&quot;&lt;/span&gt;
	&lt;span class=&quot;token string&quot;&gt;&quot;encoding/base64&quot;&lt;/span&gt;
	&lt;span class=&quot;token string&quot;&gt;&quot;fmt&quot;&lt;/span&gt;
	&lt;span class=&quot;token string&quot;&gt;&quot;log&quot;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
	buf &lt;span class=&quot;token operator&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token boolean&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; err &lt;span class=&quot;token operator&quot;&gt;:=&lt;/span&gt; rand&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;Read&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;buf&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; err &lt;span class=&quot;token operator&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;nil&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
		log&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;Fatalf&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;error while generating random string: %s&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; err&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;token comment&quot;&gt;// fmt.Println(string(buf)) // not printable&lt;/span&gt;

	printable_password &lt;span class=&quot;token operator&quot;&gt;:=&lt;/span&gt; base64&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;StdEncoding&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;EncodeToString&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;buf&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	fmt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;Println&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;generated password&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; printable_password&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;base64url&quot; tabindex=&quot;-1&quot;&gt;Base64Url &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc4648#section-5&quot;&gt;RFC4648: Section 5: Base 64 Encoding with URL and Filename Safe Alphabet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;standard Base64&lt;/a&gt; uses &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;/&lt;/code&gt; for the last 2 characters, and &lt;code&gt;=&lt;/code&gt; for padding.&lt;/li&gt;
&lt;li&gt;Base64Url uses &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;_&lt;/code&gt; for the last 2 characters, and makes padding optional.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;usage&quot; tabindex=&quot;-1&quot;&gt;Usage &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If the Base64-encoded text needs to be transmitted/saved where &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, or &lt;code&gt;=&lt;/code&gt; have special meaning, e.g. in URLs where all 3 does, then it is better to use &lt;code&gt;Base64Url&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If the Base64-encoded text needs to be transmitted/saved where &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;_&lt;/code&gt; have special meaning, then it is better to use Standard Base64.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;ref&quot; tabindex=&quot;-1&quot;&gt;Ref &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/charset-encoding/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/ut74oHojxqo?si=XBBGiFS1kw4Q7Mj2&quot;&gt;Youtube:  Unicode, in friendly terms: ASCII, UTF-8, code points, character encodings, and more &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bharatkalluri.com/posts/base64-size-increase-explanation&quot;&gt;Why is a base 64 encoded file 33% larger than the original?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/&quot;&gt;The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
	</entry>
	
	<entry>
		<title>System Design Interview Vol.2 - Chapter 6. Ad Click Event Aggregation</title>
		<link href="https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/"/>
		<updated>2023-06-18T00:00:00Z</updated>
		<id>https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/</id>
		<content type="html">&lt;p&gt;the content mainly comes from &lt;a href=&quot;https://www.amazon.com/System-Design-Interview-Insiders-Guide/dp/1736049119&quot;&gt;System Design Interview Vol.2&lt;/a&gt; - &lt;a href=&quot;https://github.com/alex-xu-system/bytebytego/blob/main/system_design_links_vol2.md#chapter-6-ad-click-event-aggregation&quot;&gt;Chapter 6. Ad Click Event Aggregation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;design a ad click event aggregation system for near-realtime data.&lt;/p&gt;
&lt;h2 id=&quot;tldr&quot; tabindex=&quot;-1&quot;&gt;TLDR &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;data pipeline / streaming: message queue&lt;/li&gt;
&lt;li&gt;aggregation service: map reduce&lt;/li&gt;
&lt;li&gt;database: read-heavy, write-heavy, no update or transaction reqiured&lt;/li&gt;
&lt;li&gt;issues:
&lt;ul&gt;
&lt;li&gt;duplicate events&lt;/li&gt;
&lt;li&gt;exactly once processing&lt;/li&gt;
&lt;li&gt;fault tolerance&lt;/li&gt;
&lt;li&gt;hotspot&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;scaling
&lt;ul&gt;
&lt;li&gt;horizontal scaling in message queue, database and aggregaiton service&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;requirement&quot; tabindex=&quot;-1&quot;&gt;Requirement &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;functional-requirement&quot; tabindex=&quot;-1&quot;&gt;Functional Requirement &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Support querying aggregated data: the number of clicks of certian ad (&lt;code&gt;ad_id&lt;/code&gt;) in last &lt;code&gt;M&lt;/code&gt; minutes&lt;/li&gt;
&lt;li&gt;Support querying aggregated data: top &lt;code&gt;N&lt;/code&gt; most clicked ad in last &lt;code&gt;M&lt;/code&gt; minutes.&lt;/li&gt;
&lt;li&gt;Support filtering by different attributes in above 2 querys.&lt;/li&gt;
&lt;li&gt;Dataset volume is FAANG scale
&lt;ul&gt;
&lt;li&gt;volume: single database is not a choice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;non-functional-requirement&quot; tabindex=&quot;-1&quot;&gt;Non-Functional Requirement &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;correctness of the aggregation result is important
&lt;ul&gt;
&lt;li&gt;as data is used for RTB and ads billing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Properly handle delayed duplicate events&lt;/li&gt;
&lt;li&gt;The system should be resilient to partial failures.&lt;/li&gt;
&lt;li&gt;End-to-end latency should be a few minutes, at most.
&lt;ul&gt;
&lt;li&gt;need realtime streaming system (instead of batch system)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;estimation&quot; tabindex=&quot;-1&quot;&gt;Estimation &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;active users: 1e9 / day&lt;/li&gt;
&lt;li&gt;assume event per user: 1 / user&lt;/li&gt;
&lt;li&gt;QPS: &lt;code&gt;(1e9 * 1) / (1e5) = 1e5&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;assume peak hour could be &lt;code&gt;5e5&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;high-level-design&quot; tabindex=&quot;-1&quot;&gt;High Level Design &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;data-model&quot; tabindex=&quot;-1&quot;&gt;Data Model &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&quot;raw-event&quot; tabindex=&quot;-1&quot;&gt;Raw Event &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ad_id&lt;/th&gt;
&lt;th&gt;timestamp&lt;/th&gt;
&lt;th&gt;user_id&lt;/th&gt;
&lt;th&gt;ip&lt;/th&gt;
&lt;th&gt;country_code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ad001&lt;/td&gt;
&lt;td&gt;2023-06-01 00:00:01&lt;/td&gt;
&lt;td&gt;user1&lt;/td&gt;
&lt;td&gt;207.148.22.22&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ad001&lt;/td&gt;
&lt;td&gt;2023-06-01 00:00:02&lt;/td&gt;
&lt;td&gt;user2&lt;/td&gt;
&lt;td&gt;209.153.55.11&lt;/td&gt;
&lt;td&gt;JP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ad002&lt;/td&gt;
&lt;td&gt;2023-06-01 00:00:02&lt;/td&gt;
&lt;td&gt;user2&lt;/td&gt;
&lt;td&gt;209.153.55.11&lt;/td&gt;
&lt;td&gt;JP&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&quot;aggregated-data&quot; tabindex=&quot;-1&quot;&gt;Aggregated Data &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ad_id&lt;/th&gt;
&lt;th&gt;timestamp_minute&lt;/th&gt;
&lt;th&gt;count&lt;/th&gt;
&lt;th&gt;filter_id&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ad001&lt;/td&gt;
&lt;td&gt;202306010000&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ad001&lt;/td&gt;
&lt;td&gt;202306010000&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0012&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ad002&lt;/td&gt;
&lt;td&gt;202306010001&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0012&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ad002&lt;/td&gt;
&lt;td&gt;202306010001&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0023&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;data under same &lt;code&gt;ad_id-timestamp_minute&lt;/code&gt; can be further group by &lt;code&gt;filter_id&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;query-api&quot; tabindex=&quot;-1&quot;&gt;Query API &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&quot;the-number-of-clicks-of-certian-ad-ad-id-in-last-m-minutes&quot; tabindex=&quot;-1&quot;&gt;the number of clicks of certian ad (&lt;code&gt;ad_id&lt;/code&gt;) in last &lt;code&gt;M&lt;/code&gt; minutes &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;API: &lt;code&gt;GET /v1/ads/{:ad_id}/aggregated_count&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;query parameters:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;from_minute&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;to_minute&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;filter_id&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;returns &lt;code&gt;ad_id&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;top-n-most-clicked-ad-in-last-m-minutes&quot; tabindex=&quot;-1&quot;&gt;top &lt;code&gt;N&lt;/code&gt; most clicked ad in last &lt;code&gt;M&lt;/code&gt; minutes &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;API: &lt;code&gt;GET /v1/ads/popular_ads&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;query parameters:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;count&lt;/code&gt;: top &lt;code&gt;N&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;window&lt;/code&gt;: last &lt;code&gt;M&lt;/code&gt; minutes&lt;/li&gt;
&lt;li&gt;&lt;code&gt;filter_id&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;returns &lt;code&gt;ad_ids&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;data-storage&quot; tabindex=&quot;-1&quot;&gt;Data Storage &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;we need to store both raw data and aggregated data&lt;/p&gt;
&lt;h4 id=&quot;comparison-between-querying-raw-data-vs-aggregated-data&quot; tabindex=&quot;-1&quot;&gt;Comparison between querying raw data vs aggregated data &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;query raw data&lt;/th&gt;
&lt;th&gt;query aggregated data&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;usage&lt;/td&gt;
&lt;td&gt;by other service&lt;/td&gt;
&lt;td&gt;this system&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pros&lt;/td&gt;
&lt;td&gt;full data set,  support ad-hoc filter&lt;/td&gt;
&lt;td&gt;smaller data set, fast query&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cons&lt;/td&gt;
&lt;td&gt;huge storage, slow query&lt;/td&gt;
&lt;td&gt;data loss&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;where&lt;/td&gt;
&lt;td&gt;cold storage&lt;/td&gt;
&lt;td&gt;database&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&quot;database-choice&quot; tabindex=&quot;-1&quot;&gt;Database Choice &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;h5&gt;analyze&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;both read heavy and write-heavy
&lt;ul&gt;
&lt;li&gt;read: refresh the aggregation data in dashboard&lt;/li&gt;
&lt;li&gt;write: insert the data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;no relation and transaction needed&lt;/li&gt;
&lt;li&gt;time series&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Choices&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;InfluxDB &lt;a href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;[1]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cassandra &lt;a href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;[2]&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Netflix use cassandra for time series database &lt;a href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;[3]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;storage engine: LSM tree &lt;a href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;[4]&lt;/a&gt; (good for write heavy)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;S3 + &lt;a href=&quot;https://www.databricks.com/glossary/what-is-parquet&quot;&gt;Parquet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;columnar storage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;high-level-design-1&quot; tabindex=&quot;-1&quot;&gt;High Level Design &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/TjtMQlbZ7i-846.avif 846w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/TjtMQlbZ7i-846.webp 846w&quot;&gt;&lt;img alt=&quot;Figure 3: High Level Design&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/TjtMQlbZ7i-846.svg&quot; width=&quot;846&quot; height=&quot;506&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;Kafka support high throughput, exact-once delivery.we can discuss exact-once delivery / atomic commit in &lt;a href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;Delivery Guarantees&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;aggregation-service&quot; tabindex=&quot;-1&quot;&gt;Aggregation Service &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&quot;the-number-of-clicks-of-certian-ad-ad-id-in-last-m-minutes-1&quot; tabindex=&quot;-1&quot;&gt;the number of clicks of certian ad (&lt;code&gt;ad_id&lt;/code&gt;) in last &lt;code&gt;M&lt;/code&gt; minutes &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/z6RTVsMZhH-949.avif 949w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/z6RTVsMZhH-949.webp 949w&quot;&gt;&lt;img alt=&quot;Figure 8 Aggregate the number of clicks&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/z6RTVsMZhH-949.svg&quot; width=&quot;949&quot; height=&quot;368&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;h2 id=&quot;deep-dive&quot; tabindex=&quot;-1&quot;&gt;Deep Dive &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;aggregation-window-and-watermark&quot; tabindex=&quot;-1&quot;&gt;Aggregation Window and Watermark &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&quot;what-is-watermarking&quot; tabindex=&quot;-1&quot;&gt;What Is Watermarking? &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;when working with real-time streaming data there will be delays between event time and processing time due to how data is ingested and whether the overall application experiences issues like downtime. Due to these potential variable delays, the engine that you use to process this data needs to have some mechanism to decide when to close the aggregate windows and produce the aggregate result.
&lt;a href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;[5]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;delivery-guarantees&quot; tabindex=&quot;-1&quot;&gt;Delivery Guarantees &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In most circumstances, &lt;strong&gt;at-least once&lt;/strong&gt; processing is good enough if a small percentage of duplicates are acceptable. However, this is not the case for our system. Differences of a few percent in data points could result in discrepancies of millions of dollars.&lt;/p&gt;
&lt;p&gt;Therefore, we recommend &lt;strong&gt;exactly-once delivery&lt;/strong&gt; for the system.&lt;/p&gt;
&lt;h3 id=&quot;data-deduplication&quot; tabindex=&quot;-1&quot;&gt;Data deduplication &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;two common sources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;client resend&lt;/li&gt;
&lt;li&gt;aggregation server outage&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/mPAtoBBGUB-1296.avif 1296w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/mPAtoBBGUB-1296.webp 1296w&quot;&gt;&lt;img alt=&quot;Figure 17 Duplicate data&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/mPAtoBBGUB-1296.svg&quot; width=&quot;1296&quot; height=&quot;1005&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;If step 6 fails, perhaps due to Aggregator outage, events from 100 to 110 are already sent to the downstream, but the new offset 110 is not persisted in upstream Kafka. In this case, a new Aggregator would consume again from offset 100, even if those events are already processed, causing duplicate data.&lt;/p&gt;
&lt;h4 id=&quot;solution&quot; tabindex=&quot;-1&quot;&gt;Solution &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/57_6r_SJe4-1791.avif 1791w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/57_6r_SJe4-1791.webp 1791w&quot;&gt;&lt;img alt=&quot;Figure 20 Distributed transaction&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/57_6r_SJe4-1791.svg&quot; width=&quot;1791&quot; height=&quot;1233&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;To achieve &lt;strong&gt;exactly-once processing&lt;/strong&gt; &lt;a href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;[6]&lt;/a&gt;, we need to put operations between step 4 to step 6 in one distributed transaction.&lt;/p&gt;
&lt;p&gt;Most common technique is two phase commit&lt;/p&gt;
&lt;h2 id=&quot;scale-the-system&quot; tabindex=&quot;-1&quot;&gt;Scale the system &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;scale-the-message-queue&quot; tabindex=&quot;-1&quot;&gt;Scale the message queue &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;partition key: use &lt;code&gt;ad_id&lt;/code&gt; as &lt;strong&gt;partition key&lt;/strong&gt;&lt;a href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;[7]&lt;/a&gt; so that an aggregation service can subscribe to all events of the same &lt;code&gt;ad_id&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;number of partitions: if more consumers need to be added, try to do it during off-peak hours to minimize the impact.&lt;/li&gt;
&lt;li&gt;Topic physical sharding: We can split the data by geography (&lt;code&gt;topic_north_america&lt;/code&gt;, &lt;code&gt;topic_europe&lt;/code&gt;, &lt;code&gt;topic_asia&lt;/code&gt;, etc.,) or by business type (&lt;code&gt;topic_web_ads&lt;/code&gt;, &lt;code&gt;topic_mobile_ads&lt;/code&gt;, etc).
&lt;ul&gt;
&lt;li&gt;Pros: increased system throughput, reduced rebalance time.&lt;/li&gt;
&lt;li&gt;Cons: extra complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;scale-the-aggregation-service&quot; tabindex=&quot;-1&quot;&gt;Scale the aggregation service &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;multi-threading: Allocate events with different &lt;code&gt;ad_id&lt;/code&gt;s to different threads.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;scale-the-database&quot; tabindex=&quot;-1&quot;&gt;Scale the database &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Cassandra natively supports horizontal scaling,&lt;/p&gt;
&lt;h3 id=&quot;hotspot-issue&quot; tabindex=&quot;-1&quot;&gt;Hotspot issue &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&quot;aggregaion-service&quot; tabindex=&quot;-1&quot;&gt;Aggregaion Service &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;some &lt;code&gt;ad_id&lt;/code&gt; might receive many more ad click events than others.&lt;/p&gt;
&lt;p&gt;Solution: dynamically allocate more node in aggregation service.&lt;/p&gt;
&lt;h4 id=&quot;kafka&quot; tabindex=&quot;-1&quot;&gt;Kafka &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The publisher specifies the topic and the partition of a message before publishing. Hence, it’s the publisher’s responsibility to ensure that the partition logic will not result in a hot partition.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.confluent.io/platform/current/kafka/monitoring.html&quot;&gt;Confluent: Monitoring Kafka with JMX&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;fault-tolerance&quot; tabindex=&quot;-1&quot;&gt;Fault tolerance &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;aggregaion service works in memory and &lt;strong&gt;could fail&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Replaying data from the beginning of Kafka is slow.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Solution: snapshot aggregaion service to safe &lt;strong&gt;current state&lt;/strong&gt; and &lt;strong&gt;event offset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/TFtUeSY7BI-558.avif 558w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/TFtUeSY7BI-558.webp 558w&quot;&gt;&lt;img alt=&quot;Figure 27 Aggregation node failover&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/TFtUeSY7BI-558.svg&quot; width=&quot;558&quot; height=&quot;309&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;h2 id=&quot;data-monitoring-and-correctness&quot; tabindex=&quot;-1&quot;&gt;Data monitoring and correctness &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;continuous-monitoring&quot; tabindex=&quot;-1&quot;&gt;Continuous monitoring &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Latency&lt;/li&gt;
&lt;li&gt;Message queue size
&lt;ul&gt;
&lt;li&gt;for aggregation servie to scale the node&lt;/li&gt;
&lt;li&gt;for application to apply back pressure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;System resources on aggregation nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;reconciliation&quot; tabindex=&quot;-1&quot;&gt;Reconciliation &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;purpose: comparing different sets of data in order to ensure data integrity.&lt;/li&gt;
&lt;li&gt;how: using a &lt;strong&gt;batch job&lt;/strong&gt; and reconciling with the real-time aggregation result.&lt;/li&gt;
&lt;li&gt;why: some events might arrive late, the result from the batch job might not match exactly with the real-time aggregation result.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/H38DsDbTtW-1033.avif 1033w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/H38DsDbTtW-1033.webp 1033w&quot;&gt;&lt;img alt=&quot;Figure 28 Final design&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/H38DsDbTtW-1033.svg&quot; width=&quot;1033&quot; height=&quot;434&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;h2 id=&quot;reference&quot; tabindex=&quot;-1&quot;&gt;Reference &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://www.influxdata.com/blog/influxdb-vs-cassandra-time-series/&quot;&gt;InfluxDB Tops Cassandra in Time Series Data &amp;amp; Metrics Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html&quot;&gt;Cassandra Time Series Data Modeling For Massive Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://netflixtechblog.com/scaling-time-series-data-storage-part-i-ec2b6d44ba39&quot;&gt;Scaling Time Series Data Storage — Part I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml/dmlManageOndisk.html&quot;&gt;Apache Cassandra™ 3.x - Storage engine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html&quot;&gt;Feature Deep Dive: Watermarking in Apache Spark Structured Streaming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://flink.apache.org/2018/02/28/an-overview-of-end-to-end-exactly-once-processing-in-apache-flink-with-apache-kafka-too/&quot;&gt;An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/kafka-streams-tables-part-2-topics-partitions-and-storage-fundamentals/#partition-events&quot;&gt;Streams and Tables in Apache Kafka: Topics, Partitions, and Storage Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
	</entry>
	
	<entry>
		<title>Dremel: A Decade of Interactive SQL Analysis at Web Scale</title>
		<link href="https://hhow09.github.io/blog/paper-dremel/"/>
		<updated>2023-06-10T00:00:00Z</updated>
		<id>https://hhow09.github.io/blog/paper-dremel/</id>
		<content type="html">&lt;p&gt;Paper: &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2023/papers/19-bigquery/p3461-melnik.pdf&quot;&gt;Dremel: A Decade of Interactive SQL Analysis at Web Scale&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot; tabindex=&quot;-1&quot;&gt;Introduction &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;the main ideas we highlight in this paper are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SQL: simple SQL query to analyze nested data&lt;/li&gt;
&lt;li&gt;Disaggregated compute and storage: decouples compute from storage&lt;/li&gt;
&lt;li&gt;In situ analysis: different compute engines can operate on same piece of data&lt;/li&gt;
&lt;li&gt;Serverless computing: fully managed internal service&lt;/li&gt;
&lt;li&gt;Columnar storage&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;embracing-sql&quot; tabindex=&quot;-1&quot;&gt;Embracing SQL &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Because SQL doesn’t scale, GFS and MapReduce became the standard ways to store and process huge datasets. However writing analysis jobs on these systems is difficult and complex. Dremel was one of the first systems to &lt;strong&gt;reintroduce SQL for Big
Data analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In Google, nearly all data passed between applications or stored on disk was in &lt;a href=&quot;https://protobuf.dev/&quot;&gt;Protocol Buffers&lt;/a&gt;. Dremel&#39;s critical innovations includes &lt;strong&gt;first-class support for structured data&lt;/strong&gt;. Dremel made it easy to query that hierarchical data with SQL.&lt;/p&gt;
&lt;h2 id=&quot;disaggregated-storage&quot; tabindex=&quot;-1&quot;&gt;Disaggregated storage &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Dremel started with shared-nothing servers and computation is coupled with storage.&lt;/p&gt;
&lt;p&gt;In 2009, Dremel was migrated to &lt;a href=&quot;https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood&quot;&gt;Borg (cluster management)&lt;/a&gt; and replicated storage. It accommodate the growing query workload and improve the utilization of the service. However &lt;strong&gt;storage and processing are still coupled&lt;/strong&gt;. Which means&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;all algorithms needed to be replication-aware&lt;/li&gt;
&lt;li&gt;resizing server also need to &lt;strong&gt;move data&lt;/strong&gt; around.&lt;/li&gt;
&lt;li&gt;Resources can only be accessed by Dremel.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Later Dremel was migrated from local disk to GFS. Performance was initially degraded. After lots of fine-tuning,  disaggregated storage outperformed the local-disk based system. There are several advantages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;improved SLOs and robustness of Dremel because GFS is fully-managed service.&lt;/li&gt;
&lt;li&gt;No more need to load from GFS onto Dremel server’s local disks.&lt;/li&gt;
&lt;li&gt;No need to resize our clusters in order to load new data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another notch of scalability and robustness was gained once Google’s file system was migrated from GFS (single-master model) to &lt;a href=&quot;https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood&quot;&gt;Colossus (distributed multi-master model)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/ZNSywBCsI0-990.avif 990w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/ZNSywBCsI0-990.webp 990w&quot;&gt;&lt;img alt=&quot;Figure 1: Disaggregated storage, memory, and compute&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/ZNSywBCsI0-990.png&quot; width=&quot;990&quot; height=&quot;512&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;h2 id=&quot;disaggregated-memory&quot; tabindex=&quot;-1&quot;&gt;Disaggregated memory &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Dremel added support for &lt;strong&gt;distributed joins&lt;/strong&gt; with &lt;strong&gt;shuffle&lt;/strong&gt; utilizing &lt;strong&gt;local RAM and disk&lt;/strong&gt; to store sorted intermediate results. However there is bottleneck in scalability and multi-tenancy due to the  coupling between the compute nodes intermediate shuffle storage.&lt;/p&gt;
&lt;p&gt;In 2014, Dremel shuffle migrated to a &lt;a href=&quot;https://cloud.google.com/blog/products/bigquery/in-memory-query-execution-in-google-bigquery&quot;&gt;new shuffle infrastructure&lt;/a&gt;. Shuffle data were stored in a distributed transient storage system. Improved peformance in-terms of latency and larger shuffle and service cost was observed.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/7rg3rOR0mF-1164.avif 1164w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/7rg3rOR0mF-1164.webp 1164w&quot;&gt;&lt;img alt=&quot;Disaggregated in-memory shuffle&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/7rg3rOR0mF-1164.png&quot; width=&quot;1164&quot; height=&quot;1012&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;h2 id=&quot;observations&quot; tabindex=&quot;-1&quot;&gt;Observations &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregation proved to be a major trend in data management.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Economies of scale&lt;/li&gt;
&lt;li&gt;Universality&lt;/li&gt;
&lt;li&gt;Higher-level APIs:  Storage access is far removed from the early block I/O APIs.&lt;/li&gt;
&lt;li&gt;Value-added repackaging&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;in-situ-data-analysis&quot; tabindex=&quot;-1&quot;&gt;In Situ Data Analysis &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The data management community finds itself today in the middle of a transition from classical data warehouses to a &lt;strong&gt;datalake-oriented architecture&lt;/strong&gt; for analytics. The trend includes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;consuming data from a variety of data sources&lt;/li&gt;
&lt;li&gt;eliminating traditional ETL-based data ingestion from an OLTP system to a data warehouse&lt;/li&gt;
&lt;li&gt;enabling a variety of compute engines to operate on the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;dremel-s-evolution-to-in-situ-analysis&quot; tabindex=&quot;-1&quot;&gt;Dremel’s evolution to in situ analysis &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Dremel’s initial design in 2006 was reminiscent of traditional DBMSs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;explicit data loading was required.&lt;/li&gt;
&lt;li&gt;the data was stored in a proprietary format.&lt;/li&gt;
&lt;li&gt;inaccessible to other tools.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As part of migrating Dremel to GFS, Dremel open the &lt;strong&gt;storage format&lt;/strong&gt; (as a library) within Google which has 2 distinguising properties: &lt;strong&gt;columnar&lt;/strong&gt;, &lt;strong&gt;self-dsecribing&lt;/strong&gt;. The &lt;strong&gt;self-dsecribing&lt;/strong&gt; feature enables interoperation between custom data transformation tools and SQL-based analytics.  MapReduce jobs could run on columnar data, write out columnar results, and those results could be immediately queried via Dremel. Users no longer had to load data into their data warehouse, any file they had in the GFS could effectively be queryable.&lt;/p&gt;
&lt;p&gt;In situ approach was evolved in two complementary directions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;adding file formats beyond original columnar format.&lt;/li&gt;
&lt;li&gt;expanding the universe of joinable data ( e.g. BigTable, Cloud Storage, Google Drive ).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;drawbacks-of-in-situ-analysis&quot; tabindex=&quot;-1&quot;&gt;Drawbacks of in situ analysis &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;users do not always want to or have the capability to manage their own data safely and securely&lt;/li&gt;
&lt;li&gt;in situ analysis means there is no opportunity to either optimize storage layout or compute statistics in the general case.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;serverless-computing&quot; tabindex=&quot;-1&quot;&gt;Serverless Computing &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;serverless-roots&quot; tabindex=&quot;-1&quot;&gt;Serverless roots &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Most traditional DBMS and data-warehouse were deployed on dedicated servers.&lt;/li&gt;
&lt;li&gt;MapReduce and Hadoop uses virtual machines but are still single-tenant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Three core ideas in Dremel which enable serverless analytics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Disaggregation of compute, storage and memory&lt;/li&gt;
&lt;li&gt;Fault Tolerance and Restartability
&lt;ul&gt;
&lt;li&gt;each subtask are deterministic and repeatable&lt;/li&gt;
&lt;li&gt;task dispatcher may need to dispatching multiple copies of the same task to alleviate unresponsive workers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Virtual Scheduling Units
&lt;ul&gt;
&lt;li&gt;Dremel scheduling logic was designed to work with abstract units of compute and memory
called slots&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;evolution-of-serverless-architecture&quot; tabindex=&quot;-1&quot;&gt;Evolution of serverless architecture &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Dremel continued to evolve its serverless capabilities, making them one of the key characteristics of Google BigQuery today.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Centralized Scheduling&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The scheduler uses the entire cluster state to make scheduling decisions which enables better utilization and isolation. (Figure 3)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/JY2-2bu-Gn-917.avif 917w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/JY2-2bu-Gn-917.webp 917w&quot;&gt;&lt;img alt=&quot;System architecture and execution inside a server
node&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/JY2-2bu-Gn-917.png&quot; width=&quot;917&quot; height=&quot;636&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shuffle Persistence Layer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;allow decoupling scheduling and execution of different stages of the query.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flexible Execution DAGs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;query coordinator builds the query plan (tree) and send to the workers&lt;/li&gt;
&lt;li&gt;Workers from the leaf stage read from the storage layer and write to the shuffle persistence layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://hhow09.github.io/img/t7gCl1YkGY-708.avif 708w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://hhow09.github.io/img/t7gCl1YkGY-708.webp 708w&quot;&gt;&lt;img alt=&quot;Shuffle-based execution plan&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://hhow09.github.io/img/t7gCl1YkGY-708.png&quot; width=&quot;708&quot; height=&quot;526&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dynamic Query Execution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;query execution plan can dynamically change during runtime based on the statistics collected during query execution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;columar-storage-for-nested-data&quot; tabindex=&quot;-1&quot;&gt;Columar Storage For Nested Data &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The main design decision behind &lt;strong&gt;repetition and definition levels encoding&lt;/strong&gt; was to encode all structure information within the column itself, so it can be accessed without reading ancestor fields.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;repetition level: specifies for repeated values whether each ancestor record is appended into or starts a new value&lt;/li&gt;
&lt;li&gt;definition level: specifies which ancestor records are absent when an optional field is absent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In 2014, migration of the storage to an improved columnar format, &lt;a href=&quot;https://cloud.google.com/blog/products/bigquery/inside-capacitor-bigquerys-next-generation-columnar-storage-format&quot;&gt;Capacitor&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;embedded-evaluation&quot; tabindex=&quot;-1&quot;&gt;Embedded evaluation &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Capacitor&lt;/strong&gt; uses a number of techniques to make filtering efficient&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Partition and predicate pruning
&lt;ul&gt;
&lt;li&gt;Various statistics are maintained about the values in each column. They are used both to eliminate partitions that are guaranteed to not contain any matching rows, and to simplify the filter by removing tautologies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vectorization: &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2019/papers/10-compression/abadi-sigmod2006.pdf&quot;&gt;Bit-Vector Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Skip-indexes
&lt;ul&gt;
&lt;li&gt;Capacitor combines column values into segments, which are compressed individually.&lt;/li&gt;
&lt;li&gt;The column header contains an index with offsets pointing to the beginning of each segment.&lt;/li&gt;
&lt;li&gt;When the filter is very selective, Capacitor uses this index to skip segments that have no hits, avoiding their decompression.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Predicate reordering
&lt;ul&gt;
&lt;li&gt;Capacitor uses a number of heuristics to make filter reordering decisions, which take into account dictionary usage, unique value cardinality,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;row-reordering&quot; tabindex=&quot;-1&quot;&gt;Row reordering &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;run-length encodings (RLE) in particular is very sensitive to &lt;strong&gt;row ordering&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Usually, row order in the table does not have significance, so Capacitor is free to permute rows to improve RLE effectiveness.&lt;/li&gt;
&lt;li&gt;Capacitor’s row reordering algorithm uses sampling and heuristics to build an approximate model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;interactive-query-latency-over-big-data&quot; tabindex=&quot;-1&quot;&gt;Interactive Query Latency Over Big Data &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;latency-reducing techniques implemented in Dremel:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Stand-by server pool&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speculative execution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;small, fine-grained task&lt;/li&gt;
&lt;li&gt;duplicate tasks to prevent slow worker&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-level execution trees&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Column-oriented schema representation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Balancing CPU and IO with lightweight compression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Approximate results&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dremel uses one-pass algorithms that work well with the multi-level execution tree architecture.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Query latency tier&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The dispatcher needed to be able to preempt the processing of parts of a query to allow a new user’s query to be processed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reuse of file operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bottleneck for achieving low latency as thousands of Dremel workers send requests to the file system master(s) for metadata and to the chunk servers for open and read operations.&lt;/li&gt;
&lt;li&gt;The most important one: reuse metadata obtained from the file system by fetching it in a batch at the root server and passing it through the execution tree to the leaf servers for data reads.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Guaranteed capacity&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a customer could reserve some capacity and use that capacity only for latency-sensitive workloads.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adaptive query scaling&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;reference&quot; tabindex=&quot;-1&quot;&gt;Reference &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/paper-dremel/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2023/papers/19-bigquery/p3461-melnik.pdf&quot;&gt;Dremel: A Decade of Interactive SQL Analysis at Web Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood&quot;&gt;BigQuery under the hood&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/products/bigquery/in-memory-query-execution-in-google-bigquery&quot;&gt;In-memory query execution in Google BigQuery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/products/bigquery/inside-capacitor-bigquerys-next-generation-columnar-storage-format&quot;&gt;Inside Capacitor, BigQuery’s next-generation columnar storage format&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
	</entry>
	
	<entry>
		<title>Distributed ID Generation Solutions</title>
		<link href="https://hhow09.github.io/blog/distributed-id-generation/"/>
		<updated>2023-05-02T00:00:00Z</updated>
		<id>https://hhow09.github.io/blog/distributed-id-generation/</id>
		<content type="html">&lt;p&gt;Using auto-increment primary keys from traditional databases as ID for distributed systems can be inefficient and vulnerable to being predicted and analyzed.&lt;/p&gt;
&lt;p&gt;Nowadays, distributed systems require unique global identifiers; it is an essential task in distributed computing with growing internet usage.&lt;/p&gt;
&lt;h2 id=&quot;requirement&quot; tabindex=&quot;-1&quot;&gt;Requirement &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;functional-requirement&quot; tabindex=&quot;-1&quot;&gt;Functional Requirement &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;id should be globally unique&lt;/li&gt;
&lt;li&gt;low latency&lt;/li&gt;
&lt;li&gt;each node can generate id independently&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;nice-to-have-features&quot; tabindex=&quot;-1&quot;&gt;Nice to have features &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;monotonicity (sortable by time)&lt;/li&gt;
&lt;li&gt;unpredictable&lt;/li&gt;
&lt;li&gt;high availability. Since an ID generator is a mission-critical system, it must be highly available.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;popular-solutions&quot; tabindex=&quot;-1&quot;&gt;Popular Solutions &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;UUID V4&lt;/li&gt;
&lt;li&gt;Nano ID&lt;/li&gt;
&lt;li&gt;ULID&lt;/li&gt;
&lt;li&gt;KSUID&lt;/li&gt;
&lt;li&gt;Mongdb objectID&lt;/li&gt;
&lt;li&gt;Snowflake ID&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;overview&quot; tabindex=&quot;-1&quot;&gt;Overview &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Designing a distributed ID generation scheme can be divided into two steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generate the binary ID, usually selected from pseudo-random numbers, time, and node ID.&lt;/li&gt;
&lt;li&gt;Convert the binary ID into a human-readable and easily transmittable string text.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;binary-id-generation&quot; tabindex=&quot;-1&quot;&gt;Binary ID Generation &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;1-prng-pseudo-random-number-generator&quot; tabindex=&quot;-1&quot;&gt;1. PRNG (pseudo random number generator) &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;UUID v4 (128 bit)&lt;/li&gt;
&lt;li&gt;Nano ID (no standard length)&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;&lt;/th&gt;
&lt;th&gt;random number (bits)&lt;/th&gt;
&lt;th&gt;total (bits)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;UUID v4&lt;/td&gt;
&lt;td&gt;122&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;Nano ID&lt;/td&gt;
&lt;td&gt;126&lt;/td&gt;
&lt;td&gt;126&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&quot;uuid-2&quot; tabindex=&quot;-1&quot;&gt;UUID &lt;a href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;[2]&lt;/a&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;UUIDv4 consists of 128 bits, which are typically represented as 32 hexadecimal characters in the following format: xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx, where the digit &amp;quot;4&amp;quot; in the third group signifies the version (i.e., UUIDv4) and the digit &amp;quot;y&amp;quot; in the fourth group specifies the variant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;pros&quot; tabindex=&quot;-1&quot;&gt;Pros &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generating UUID is simple. No coordination between servers is needed so there will not be any synchronization issues.&lt;/li&gt;
&lt;li&gt;The system is easy to scale because each web server is responsible for generating IDs they consume. ID generator can easily scale with web servers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;cons&quot; tabindex=&quot;-1&quot;&gt;Cons &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;IDs are 128 bits long, but our requirement is 64 bits.&lt;/li&gt;
&lt;li&gt;IDs do not go up with time.&lt;/li&gt;
&lt;li&gt;IDs could be non-numeric.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Chance of duplicate ID&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;With the sheer number of possible combinations (2^128), it would be almost impossible to generate a duplicate unless you are generating trillions of IDs every second, for many years.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;nano-id-3&quot; tabindex=&quot;-1&quot;&gt;Nano ID &lt;a href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;[3]&lt;/a&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;A tiny, secure, URL-friendly, unique string ID generator.
Small. 130 bytes (minified and gzipped). No dependencies. Size Limit controls the size.
Safe. It uses hardware random generator. Can be used in clusters.
Short IDs. It uses a larger alphabet than UUID (A-Za-z0-9_-). So ID size was reduced from 36 to 21 symbols.
Portable. Nano ID was ported to 20 programming languages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;2-timestamp-prng&quot; tabindex=&quot;-1&quot;&gt;2. Timestamp + PRNG &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;&lt;/th&gt;
&lt;th&gt;timestamp (bits)&lt;/th&gt;
&lt;th&gt;random number (bits)&lt;/th&gt;
&lt;th&gt;total (bits)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;ULID&lt;/td&gt;
&lt;td&gt;48&lt;/td&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;KSUID&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&quot;ulid-4&quot; tabindex=&quot;-1&quot;&gt;ULID &lt;a href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;[4]&lt;/a&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;128-bit compatibility with UUID&lt;/li&gt;
&lt;li&gt;Lexicographically sortable!
&lt;ul&gt;
&lt;li&gt;timestamp: UNIX-time in milliseconds&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;No special characters (URL safe)&lt;/li&gt;
&lt;li&gt;Monotonic sort order (correctly detects and handles the same millisecond)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;ksuid-4&quot; tabindex=&quot;-1&quot;&gt;KSUID &lt;a href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;[4]&lt;/a&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;There are numerous methods for generating unique identifiers, so why KSUID?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Naturally ordered by generation time&lt;/li&gt;
&lt;li&gt;Collision-free, coordination-free, dependency-free&lt;/li&gt;
&lt;li&gt;Highly portable representations&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;3-timestamp-node-id-monotonic-counter&quot; tabindex=&quot;-1&quot;&gt;3. Timestamp + Node ID + Monotonic Counter &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;&lt;/th&gt;
&lt;th&gt;timestamp (bits)&lt;/th&gt;
&lt;th&gt;Node Id (bits)&lt;/th&gt;
&lt;th&gt;Process Id (bits)&lt;/th&gt;
&lt;th&gt;Counter (bits)&lt;/th&gt;
&lt;th&gt;total (bits)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;Snowflake ID&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;MongoDB ObjectID&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;id is unique across different node and process&lt;/li&gt;
&lt;li&gt;counter ensure the uniqueness on same process of same node&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;security-concern&quot; tabindex=&quot;-1&quot;&gt;Security Concern &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;since the structure of id, the entropy of id is quite small, which means &lt;a href=&quot;https://github.com/andresriancho/mongo-objectid-predict&quot;&gt;easy to predict&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;It is dangerous through &lt;a href=&quot;https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/05-Authorization_Testing/04-Testing_for_Insecure_Direct_Object_References&quot;&gt;IDOR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;snowflake-id-6&quot; tabindex=&quot;-1&quot;&gt;Snowflake ID &lt;a href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;[6]&lt;/a&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;timestamp: Epoch in milliseconds precision
&lt;ul&gt;
&lt;li&gt;That gives us &lt;strong&gt;69&lt;/strong&gt; years with respect to a &lt;strong&gt;custom epoch&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Node ID: 10 bits, this gives us 1024 nodes/machines.&lt;/li&gt;
&lt;li&gt;Local counter: 12 bits, The counter’s max value would be 4095.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;mongodb-objectid-7&quot; tabindex=&quot;-1&quot;&gt;MongoDB ObjectID &lt;a href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;[7]&lt;/a&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The object ID is generated by the MongoDB driver instead of the database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The 12-byte ObjectId consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A 4-byte timestamp, representing the ObjectId&#39;s creation, measured in seconds since the Unix epoch.&lt;/li&gt;
&lt;li&gt;A 5-byte random value generated once per process. This random value is unique to the machine and process.&lt;/li&gt;
&lt;li&gt;A 3-byte incrementing counter, initialized to a random value.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;types-of-data-source&quot; tabindex=&quot;-1&quot;&gt;Types of data source &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;pseudo-random-number&quot; tabindex=&quot;-1&quot;&gt;Pseudo random number &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&quot;packages&quot; tabindex=&quot;-1&quot;&gt;Packages &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://pkg.go.dev/crypto/rand&quot;&gt;crypto/rand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://pkg.go.dev/math/rand&quot;&gt;math/rand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;considerations&quot; tabindex=&quot;-1&quot;&gt;Considerations &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;collisoin: longer bits gives lower chance of collision&lt;/li&gt;
&lt;li&gt;unpredictability: use &lt;code&gt;crypto/rand&lt;/code&gt; to lower unpredictability&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;reference&quot; tabindex=&quot;-1&quot;&gt;Reference &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-id-generation/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://catcat.cc/post/2020-09-19/&quot;&gt;6 个流行的分布式 ID 方案之间的对决&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc4122&quot;&gt;UUID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ai/nanoid&quot;&gt;Nano ID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ulid/spec&quot;&gt;ULID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/segmentio/ksuid&quot;&gt;KSUID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/bwmarrin/snowflake&quot;&gt;Snowflake ID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.mongodb.com/docs/manual/reference/method/ObjectId/&quot;&gt;MongoDB ObjectID&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
	</entry>
	
	<entry>
		<title>Distributed System Index</title>
		<link href="https://hhow09.github.io/blog/distributed-system-index/"/>
		<updated>2023-04-23T00:00:00Z</updated>
		<id>https://hhow09.github.io/blog/distributed-system-index/</id>
		<content type="html">&lt;h2 id=&quot;course-books&quot; tabindex=&quot;-1&quot;&gt;Course / Books &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/&quot;&gt;Designing Data Intensive Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://pdos.csail.mit.edu/6.824/index.html&quot;&gt;MIT 6.824: Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.cl.cam.ac.uk/teaching/2122/ConcDisSys/&quot;&gt;CAM: Concurrent and Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB&quot;&gt;Distributed Systems lecture series: Martin Kleppmann&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.cl.cam.ac.uk/teaching/2122/ConcDisSys/dist-sys-notes.pdf&quot;&gt;course notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://martinfowler.com/articles/patterns-of-distributed-systems/&quot;&gt;Martin Fowler: Patterns of Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;cap-theorem&quot; tabindex=&quot;-1&quot;&gt;CAP Theorem &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/henryr/cap-faq&quot;&gt;henryr/cap-faq&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;issues&quot; tabindex=&quot;-1&quot;&gt;Issues &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;split brain&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;consistency-models&quot; tabindex=&quot;-1&quot;&gt;Consistency Models &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;overview: &lt;a href=&quot;https://jepsen.io/consistency&quot;&gt;jepsen/consistency&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;linearizability&quot; tabindex=&quot;-1&quot;&gt;linearizability &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h3 id=&quot;causal-consistency&quot; tabindex=&quot;-1&quot;&gt;causal consistency &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.exhypothesi.com/clocks-and-causality/&quot;&gt;Clocks and Causality - Ordering Events in Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jepsen.io/consistency/models/causal&quot;&gt;Jepsen: Causal Consistency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vkontech.com/causal-consistency-guarantees-case-studies/#What_is_Causal_Consistency&quot;&gt;Causal Consistency Guarantees – Case Studies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://timilearning.com/posts/mit-6.824/lecture-17-cops/&quot;&gt;MIT 6.824: Lecture 17 - Causal Consistency, COPS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/&quot;&gt;Mongodb: Causal Consistency and Read and Write Concerns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;eventual-consistency&quot; tabindex=&quot;-1&quot;&gt;eventual consistency &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h2 id=&quot;replication&quot; tabindex=&quot;-1&quot;&gt;Replication &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Designing Data Intensive Applications Chapter 5 Replication&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;models-of-replication&quot; tabindex=&quot;-1&quot;&gt;Models of Replication &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Leaders and Followers&lt;/li&gt;
&lt;li&gt;Multi-Leader Replication&lt;/li&gt;
&lt;li&gt;Leaderless Replication&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;write-ahead-log&quot; tabindex=&quot;-1&quot;&gt;Write Ahead Log &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html&quot;&gt;Martin Fowler: Write-Ahead Log&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.oreilly.com/library/view/distributed-services-with/9781680508376/f_0025.xhtml&quot;&gt;Distributed Services with Go: Chapter 3 Write a Log Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/wal-intro.html&quot;&gt;PostGreSQL: 30.3. Write-Ahead Logging (WAL)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.mongodb.com/docs/manual/core/replica-set-oplog/&quot;&gt;MongoDB Oplog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;postgresql&quot; tabindex=&quot;-1&quot;&gt;PostGreSQL &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;physical v.s. Logical streaming replication&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;mongodb&quot; tabindex=&quot;-1&quot;&gt;MongoDB &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.mongodb.com/docs/manual/replication/&quot;&gt;MongoDB Manual: Replication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;partition&quot; tabindex=&quot;-1&quot;&gt;Partition &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id=&quot;raft-consensus-algorithm&quot; tabindex=&quot;-1&quot;&gt;Raft (Consensus Algorithm) &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://raft.github.io/&quot;&gt;The Raft Consensus Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://thesecretlivesofdata.com/raft/&quot;&gt;The Secret Lives of Data: Raft&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/uXEYuDwm7e4&quot;&gt;Distributed Systems 6.2: Raft&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.cl.cam.ac.uk/teaching/2122/ConcDisSys/dist-sys-notes.pdf&quot;&gt;University of Cambridge: Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;rum-conjecture&quot; tabindex=&quot;-1&quot;&gt;RUM Conjecture &lt;a class=&quot;header-anchor&quot; href=&quot;https://hhow09.github.io/blog/distributed-system-index/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The Trade offs Behind Modern Storage Systems&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://daslab.seas.harvard.edu/rum-conjecture/&quot;&gt;The RUM Conjecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://edward-huang.com/distributed-system/2021/01/24/the-trade-offs-behind-modern-storage-systems/&quot;&gt;The Trade offs Behind Modern Storage Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/wxcCHvQeZ-U&quot;&gt;Youtube: Algorithms behind Modern Storage Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
	</entry>
</feed>
