{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "hhow09&#39;s Blog",
	"language": "en",
	"home_page_url": "https://hhow09.github.io/",
	"feed_url": "https://hhow09.github.io/feed/feed.json",
	"description": "sofware engineering stuff",
	"author": {
		"name": "HH",
		"url": "https://hhow09.github.io/about-me/"
	},
	"items": [
		{
			"id": "https://hhow09.github.io/blog/oauth2-refresh-token/",
			"url": "https://hhow09.github.io/blog/oauth2-refresh-token/",
			"title": "OAuth 2.0 - Refresh Token and Rotation",
			"content_html": "<h2 id=\"specification\" tabindex=\"-1\">Specification <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h2>\n<ol>\n<li>\n<p>Refresh tokens are credentials used to obtain access tokens. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-1.5\">RFC 6749 1.5</a></em></p>\n<ul>\n<li>(1) when current access token expires.</li>\n<li>(2) to obtain additional access tokens with identical or narrower scope.</li>\n</ul>\n</li>\n<li>\n<p>Issuing a refresh token is optional at the discretion of the authorization server. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-1.5\">Oauth6749 1.5</a></em></p>\n</li>\n<li>\n<p>Refresh tokens MUST be kept confidential in transit and storage, and shared only among the authorization server and the client to whom the refresh tokens were issued. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-10.4\">RFC 6749 10.4</a></em></p>\n</li>\n<li>\n<p>The authorization server MUST maintain the binding between a refresh token and the client to whom it was issued. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-10.4\">RFC 6749 10.4</a></em></p>\n</li>\n<li>\n<p>Refresh tokens MUST only be transmitted using TLS <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-10.4\">RFC 6749 10.4</a></em></p>\n</li>\n<li>\n<p>The authorization server MUST verify the binding between the refresh token and client identity. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-10.4\">RFC 6749 10.4</a></em></p>\n</li>\n<li>\n<p>When client authentication is not possible, the authorization server SHOULD deploy other means to detect refresh token abuse. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-10.4\">RFC 6749 10.4</a></em></p>\n<ul>\n<li>e.g. <a href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">Refresh Token Rotation</a></li>\n</ul>\n</li>\n<li>\n<p>If a refresh token is compromised and subsequently used by both the attacker and the legitimate client, one of them will present an  invalidated refresh token, which will inform the authorization server of the breach. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-10.4\">RFC 6749 10.4</a></em></p>\n</li>\n<li>\n<p>The authorization server MUST ensure that refresh tokens cannot be generated, modified, or guessed to produce valid refresh tokens by unauthorized parties. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-10.4\">RFC 6749 10.4</a></em></p>\n<ul>\n<li>cannot be modified: signing</li>\n<li>cannot be guessed: encryption</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"other-specifications\" tabindex=\"-1\">Other Specifications <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h3>\n<ol>\n<li>OAuth 2.0 Token Revocation <em>[REF]<a href=\"https://datatracker.ietf.org/doc/html/rfc7009\">RFC 7009</a></em></li>\n<li>Refresh Token Expiration\n<ul>\n<li>The refresh token has not been used for six months. <em>[REF]<a href=\"https://developers.google.com/identity/protocols/oauth2?hl=en#5.-refresh-the-access-token,-if-necessary\">Google OAuth</a></em></li>\n<li>LinkedIn offers programmatic refresh tokens that are valid for a fixed length of time. <em>[REF]<a href=\"https://learn.microsoft.com/en-us/linkedin/shared/authentication/programmatic-refresh-tokens\">Google Refresh Tokens with OAuth 2.0</a></em></li>\n</ul>\n</li>\n</ol>\n<h2 id=\"refresh-access-token\" tabindex=\"-1\">Refresh Access Token <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h2>\n<h3 id=\"why\" tabindex=\"-1\">Why <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h3>\n<ul>\n<li>access token usually issued for a limited time.</li>\n<li>In the scenario of an expiring access token, your application has two alternatives:\n<ul>\n<li>Ask the users of your application to re-authenticate each time an access token expires.</li>\n<li>The authorization server automatically issues a new access token once it expires.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"how-to-refreshing-an-access-token\" tabindex=\"-1\">How to Refreshing an Access Token ? <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h3>\n<h4 id=\"client-request\" tabindex=\"-1\">Client Request <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h4>\n<pre><code>     POST /token HTTP/1.1\n     Host: server.example.com\n     Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW\n     Content-Type: application/x-www-form-urlencoded\n     grant_type=refresh_token&amp;refresh_token=tGzv3JOkF0XG5Qx2TlKWIA\n</code></pre>\n<h4 id=\"auth-server-ref-rfc-6749-6\" tabindex=\"-1\">Auth Server <em>[REF]<a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-6\">RFC 6749 6</a></em> <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h4>\n<ol>\n<li>MUST authenticate the client if client authentication is included</li>\n<li>MUST validate the refresh token.</li>\n<li>IF valid and authorized, the authorization server issues an access token</li>\n<li>The authorization server MAY issue a new refresh token, in which case the client MUST discard the old refresh token and replace it with the new refresh token.</li>\n<li>The authorization server MAY revoke the old refresh token after issuing a new refresh token to the client.</li>\n<li>If a new refresh token is issued, the refresh token scope MUST be identical to that of the refresh token included by the client in the request.</li>\n</ol>\n<h4 id=\"flow\" tabindex=\"-1\">Flow <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h4>\n<pre><code>  +--------+                                           +---------------+\n  |        |--(A)------- Authorization Grant ---------&gt;|               |\n  |        |                                           |               |\n  |        |&lt;-(B)----------- Access Token -------------|               |\n  |        |               &amp; Refresh Token             |               |\n  |        |                                           |               |\n  |        |                            +----------+   |               |\n  |        |--(C)---- Access Token ----&gt;|          |   |               |\n  |        |                            |          |   |               |\n  |        |&lt;-(D)- Protected Resource --| Resource |   | Authorization |\n  | Client |                            |  Server  |   |     Server    |\n  |        |--(E)---- Access Token ----&gt;|          |   |               |\n  |        |                            |          |   |               |\n  |        |&lt;-(F)- Invalid Token Error -|          |   |               |\n  |        |                            +----------+   |               |\n  |        |                                           |               |\n  |        |--(G)----------- Refresh Token -----------&gt;|               |\n  |        |                                           |               |\n  |        |&lt;-(H)----------- Access Token -------------|               |\n  +--------+           &amp; Optional Refresh Token        +---------------+\n\n               Figure 2: Refreshing an Expired Access Token\n</code></pre>\n<p><em>[REF]<a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-1.5\">RFC 6749 1.5</a></em></p>\n<h2 id=\"refresh-token-rotation\" tabindex=\"-1\">Refresh Token Rotation <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h2>\n<ol>\n<li>Refresh token rotation is intended to automatically detect and prevent attempts to use the same refresh token in parallel from different apps/devices. This happens if a token gets stolen from the client and is subsequently used by both the attacker and the legitimate client. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6819#section-5.2.2.3\">RFC 6819 5.2.2.3</a></em></li>\n<li>The basic idea is to change the refresh token value with every refresh request in order to detect attempts to obtain access tokens using old refresh tokens. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6819#section-5.2.2.3\">RFC 6819 5.2.2.3</a></em></li>\n<li>It's a way to prevent token reuse according to <a href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">Specification 7</a></li>\n</ol>\n<h3 id=\"implementatino-notes\" tabindex=\"-1\">Implementatino Notes <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h3>\n<ol>\n<li>In accessing data storage layer, (1) New Refresh Token and (2) Invalidate Refresh Token should be atomic.</li>\n<li><a href=\"https://developer.okta.com/docs/guides/refresh-tokens/main/#refresh-token-rotation\">Okta: Refresh token rotation</a></li>\n<li><a href=\"https://auth0.com/docs/secure/tokens/refresh-tokens/refresh-token-rotation\">Auth0: Refresh Token Rotation</a></li>\n</ol>\n<h2 id=\"refresh-token-reuse-detection\" tabindex=\"-1\">Refresh token reuse detection <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h2>\n<ol>\n<li>If a previously used refresh token is used again with the token request, the authorization server automatically detects the attempted reuse of the refresh token. <em>[REF]<a href=\"https://developer.okta.com/docs/guides/refresh-tokens/main/#set-up-your-application\">Okta: Refresh token reuse detection</a></em></li>\n<li>As a result, Okta immediately invalidates the most recently issued refresh token and all access tokens issued since the user authenticated. This protects your application from token compromise and replay attacks. <em>[REF]<a href=\"https://developer.okta.com/docs/guides/refresh-tokens/main/#set-up-your-application\">Okta: Refresh token reuse detection</a></em>\n<ul>\n<li>Since the authorization server cannot determine whether the attacker or the legitimate client is trying to access, in case of such an access attempt the valid refresh token and the access authorization associated with it are both revoked. <em>[REF] <a href=\"https://datatracker.ietf.org/doc/html/rfc6819#section-5.2.2.3\">RFC 6819 5.2.2.3</a></em></li>\n</ul>\n</li>\n</ol>\n<h3 id=\"reuse-scenario\" tabindex=\"-1\">Reuse Scenario <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h3>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/1-cV09DgQ2-2999.avif 2999w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/1-cV09DgQ2-2999.webp 2999w\"><img alt=\"Scenario 1: Refresh Token Reuse\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/1-cV09DgQ2-2999.png\" width=\"2999\" height=\"1687\"></picture></p>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/KIbZ25k-l1-2999.avif 2999w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/KIbZ25k-l1-2999.webp 2999w\"><img alt=\"Scenario 2: Refresh Token Reuse\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/KIbZ25k-l1-2999.png\" width=\"2999\" height=\"1687\"></picture></p>\n<blockquote>\n<p>In these scenarios, the reuse of a refresh token triggers all kinds of alarms with the authorization server. Refresh token reuse likely means that a second party is trying to use a stolen refresh token. In response to this reuse, the authorization server immediately revokes the reused refresh token, along with all descendant tokens. Concretely, all refresh tokens that have ever been derived from the reused refresh token become invalid.</p>\n</blockquote>\n<p>[REF]<a href=\"https://www.pingidentity.com/en/resources/blog/post/refresh-token-rotation-spa.html\">A Critical Analysis of Refresh Token Rotation in Single-page Applications</a></p>\n<h3 id=\"issue-client-retry-v-s-replay-attack\" tabindex=\"-1\">Issue: Client Retry v.s. Replay attack <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h3>\n<ol>\n<li>\n<p>Token reuse detection can sometimes impact the user experience. For example, when users with poor network connections access apps, new tokens issued by Okta might not reach the client app. As a result, the client might want to reuse the refresh token to get new tokens. <em>[REF]<a href=\"https://developer.okta.com/docs/guides/refresh-tokens/main/#grace-period-for-token-rotation\">Okta: Grace period for token rotation</a></em></p>\n</li>\n<li>\n<p>According to <a href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">Specification 8</a>, Refresh token reuse detection should be implemented for preventing <a href=\"https://auth0.com/docs/secure/security-guidance/prevent-threats#replay-attacks\">replay attack</a>.</p>\n</li>\n<li>\n<p>Without enforcing sender-constraint, it’s impossible for the authorization server to know which actor is legitimate or malicious in the event of a replay attack. <em>[REF]<a href=\"https://auth0.com/docs/secure/tokens/refresh-tokens/refresh-token-rotation\">Auth0: Refresh Token Rotation</a></em></p>\n<ul>\n<li>If retry is allowed, it means somehow security is sacrificed in some degree.</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"solution-from-okta-grace-period\" tabindex=\"-1\">Solution from Okta: Grace period <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h4>\n<ul>\n<li>Okta offers a grace period when you configure refresh token rotation. After the refresh token is rotated, the previous token remains valid for the configured amount of time to allow clients to get the new token.</li>\n<li>The default number of seconds for the Grace period for token rotation is set to <strong>30</strong> seconds. You can change the value to any number from 0-60 seconds. After the refresh token is rotated, the previous token remains valid for this amount of time to allow clients to get the new token.</li>\n<li>[REF]<a href=\"https://developer.okta.com/docs/guides/refresh-tokens/main/#grace-period-for-token-rotation\">Grace period for token rotation</a></li>\n</ul>\n<h4 id=\"solution-from-auth0-grace-period\" tabindex=\"-1\">Solution from Auth0: Grace period <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h4>\n<ul>\n<li>\n<p>Enter Reuse Interval (in seconds) for the refresh token to account for leeway time between request and response before triggering automatic reuse detection. This interval helps to avoid concurrency issues when exchanging the rotating refresh token multiple times within a given timeframe. During the leeway window the breach detection features don't apply and a new rotating refresh token is issued. Only the previous token can be reused; if the second-to-last one is exchanged, breach detection will be triggered.</p>\n</li>\n<li>\n<p>[REF]<a href=\"https://auth0.com/docs/secure/tokens/refresh-tokens/configure-refresh-token-rotation\">Auth0: Configure Refresh Token Rotation</a></p>\n</li>\n</ul>\n<h4 id=\"solution-2-revokaion-on-use\" tabindex=\"-1\">Solution 2: Revokaion-On-Use <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h4>\n<ol>\n<li>\n<p>Allow client retry with same refresh token</p>\n<ul>\n<li>Keep track of (1) one parent toke (RT1) and (2) a pool of child tokens.</li>\n<li>parent token means the latest token's</li>\n<li>child token means the retry-results of same parent. But <strong>we don't know if they actually successfully recieved by client</strong>.</li>\n<li>parent token (RT1) could generate multiple child refresh tokens and access tokens in pairs ((RT2, AT2), (RT2-1, AT2-1)...). Keep track of them.</li>\n</ul>\n</li>\n<li>\n<p>Invalidate other refresh tokens AFTER (1) one of sibling refresh token or (2) the corresponding <code>access_token</code> once used.</p>\n<ul>\n<li>Once <code>AT2-1</code> is used, (1) sibling (RT2, AT2), (RT2-2, AT2)... should be revoked\n<ul>\n<li>because it ensures client already received one of the retries.</li>\n</ul>\n</li>\n<li>Once <code>RT2-1</code> is used (in next refresh), (1) sibling (RT2, AT2), (RT2-2, AT2)... should be revoked (2) (RT3, AT3) should be generated (3) <code>RT2-1</code> should considered used and become parent.</li>\n</ul>\n</li>\n<li>\n<p>[REF]<a href=\"https://devforum.zoom.us/t/how-to-protect-against-losing-refresh-token-response/10375/1\">How to protect against losing <code>refresh_token</code> response?</a></p>\n</li>\n<li>\n<p>Concurrent token READ (token introspection) and WRITE (refresh token) should be handled carefully.</p>\n</li>\n</ol>\n<h2 id=\"open-source-oauth-server\" tabindex=\"-1\">Open Source OAuth Server <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/oauth2-refresh-token/\">#</a></h2>\n<ul>\n<li><a href=\"https://github.com/ory/fosite.git\">https://github.com/ory/fosite.git</a></li>\n<li><a href=\"https://github.com/supertokens/supertokens-core\">https://github.com/supertokens/supertokens-core</a></li>\n<li><a href=\"https://github.com/keycloak/keycloak/blob/main/services/src/main/java/org/keycloak/jose/jws/DefaultTokenManager.java\">https://github.com/keycloak/keycloak/blob/main/services/src/main/java/org/keycloak/jose/jws/DefaultTokenManager.java</a></li>\n</ul>\n",
			"date_published": "2023-12-08T00:00:00Z"
		}
		,
		{
			"id": "https://hhow09.github.io/blog/charset-encoding/",
			"url": "https://hhow09.github.io/blog/charset-encoding/",
			"title": "Character Sets and Encodings",
			"content_html": "<h2 id=\"ascii\" tabindex=\"-1\">ASCII <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h2>\n<ul>\n<li><a href=\"https://datatracker.ietf.org/doc/html/rfc20\">RFC 20 ASCII format for Network Interchange</a></li>\n<li>ASCII is a character set\n<ul>\n<li>Codes 0-31 are unprintable control codes and are used to control peripherals such as printers.</li>\n<li>Codes 32-127 are called printable characters, they are for all the <a href=\"https://hhow09.github.io/blog/charset-encoding/\">different variations of the ASCII table</a></li>\n</ul>\n</li>\n<li>use 8 bits to represent individual characters. (7-bit in early age)</li>\n<li><a href=\"https://datatracker.ietf.org/doc/html/rfc2616\">HTTP 1.1</a> uses <code>US-ASCII</code> as basic character set for the <a href=\"https://datatracker.ietf.org/doc/html/rfc2616#section-4.2\">request line</a> in requests, the <a href=\"https://datatracker.ietf.org/doc/html/rfc2616#section-6.1.1\">status line</a> in responses (except the <a href=\"https://datatracker.ietf.org/doc/html/rfc2616#section-6.1.1\">reason phrase</a>) and the <a href=\"https://datatracker.ietf.org/doc/html/rfc2616#section-4.2\">field names</a> but allows any octet in the field values and the <a href=\"https://datatracker.ietf.org/doc/html/rfc2616#section-4.3\">message body</a>.</li>\n</ul>\n<h3 id=\"eascii\" tabindex=\"-1\">EASCII <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h3>\n<ul>\n<li>extended ASCII codes</li>\n</ul>\n<h2 id=\"unicode\" tabindex=\"-1\">UNICODE <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h2>\n<ul>\n<li><a href=\"https://www.rfc-editor.org/rfc/rfc5198\">RFC 5198: Unicode Format for Network Interchange</a></li>\n<li><a href=\"https://home.unicode.org/\">unicode org</a></li>\n<li>\n<blockquote>\n<p>The Unicode Standard refers to the standard character set that represents all natural language characters. Unicode can encode up to roughly 1.1 million characters, allowing it to support all of the world’s languages and scripts in a single, universal standard.</p>\n</blockquote>\n</li>\n<li>UNICODE is <code>ASCII</code> compatible (<code>U+0000</code> to <code>U+007F</code>)</li>\n</ul>\n<h2 id=\"utf-8\" tabindex=\"-1\">UTF-8 <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h2>\n<ul>\n<li><a href=\"https://datatracker.ietf.org/doc/html/rfc3629\">RFC 3629: UTF-8, a transformation format of ISO 10646</a></li>\n<li>UTF-8 is defined by the Unicode Standard [<a href=\"https://datatracker.ietf.org/doc/html/rfc3629#ref-UNICODE\">UNICODE</a>]</li>\n<li>In UTF-8, characters from the <code>U+0000..U+10FFFF</code> range  are encoded using sequences of 1 to 4 octets.</li>\n</ul>\n<pre><code>   Char. number range  |        UTF-8 octet sequence\n      (hexadecimal)    |              (binary)\n   --------------------+------------------------------------\n   0000 0000-0000 007F | 0xxxxxxx\n   0000 0080-0000 07FF | 110xxxxx 10xxxxxx\n   0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx\n   0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx\n</code></pre>\n<h3 id=\"example\" tabindex=\"-1\">Example <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h3>\n<ul>\n<li>HTML header: <code>Content-Type: text/plain; charset=&quot;UTF-8&quot;</code></li>\n<li>Golang: <a href=\"https://go.dev/ref/spec#Source_code_representation\">Rune literals</a> use UTF-8</li>\n<li>Rust: <a href=\"https://doc.rust-lang.org/std/string/struct.String.html\">Struct std::string::String</a> A UTF-8–encoded, growable string.</li>\n</ul>\n<h2 id=\"base64\" tabindex=\"-1\">Base64 <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h2>\n<ul>\n<li>\n<p>RFC: <a href=\"https://datatracker.ietf.org/doc/html/rfc4648#section-4\">rfc4648#section-4</a></p>\n</li>\n<li>\n<p>24 bits byte sequence can be represented by four <strong>6-bit Base64 digits</strong>.</p>\n<ul>\n<li>4 chars are used to represent <code>4 * 6 = 24 bits = 3 bytes</code> (if we ignore the <a href=\"https://datatracker.ietf.org/doc/html/rfc4648#section-3.2\">padding</a> and round-up detail)</li>\n<li>3-char string will become 4-char string after the encoding, which the means size will increase by about 33%.</li>\n</ul>\n</li>\n<li>\n<p>Used when there was a need to encode <strong>binary data</strong> so that it can be stored and transferred over mediums that primarily designed to deal with ASCII text. E-Mail attachments are sent out as base64 encoded strings.</p>\n</li>\n<li>\n<p>IS: case sensitive</p>\n</li>\n<li>\n<p>In Unix system, <a href=\"https://en.wikipedia.org/wiki/Crypt_(C)\">crypt()</a> uses a special Base64-type of encoding. It uses <code>./0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</code> to encode the hashed password.</p>\n</li>\n</ul>\n<h3 id=\"example-1\" tabindex=\"-1\">Example: <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h3>\n<ul>\n<li>In binary, &quot;cat&quot; is <code>01100011 01100001 01110100</code> ( 3 bytes)</li>\n<li>base 64 &quot;cat&quot; would be<pre><code>011000 110110 000101 110100\n|      |      |      |\nY      2      F      0\n</code></pre>\n</li>\n</ul>\n<h3 id=\"example-generate-random-password\" tabindex=\"-1\">Example - Generate random password <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h3>\n<blockquote>\n<p>Language-specific characters are typically avoided by password generators because they would not be universally available (US keyboards don't have accented characters, for instance). So don't take their omission from these tools as an indication that they might be weak or problematic.\n- <a href=\"https://security.stackexchange.com/questions/225346/is-it-bad-to-use-special-characters-in-passwords\">Is it bad to use special characters in passwords? [duplicate]</a></p>\n</blockquote>\n<pre class=\"language-go\" tabindex=\"0\"><code class=\"language-go\"><span class=\"token keyword\">package</span> main\n<span class=\"token keyword\">import</span> <span class=\"token punctuation\">(</span>\n\t<span class=\"token string\">\"crypto/rand\"</span>\n\t<span class=\"token string\">\"encoding/base64\"</span>\n\t<span class=\"token string\">\"fmt\"</span>\n\t<span class=\"token string\">\"log\"</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">func</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n\tbuf <span class=\"token operator\">:=</span> <span class=\"token function\">make</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token builtin\">byte</span><span class=\"token punctuation\">,</span> <span class=\"token number\">32</span><span class=\"token punctuation\">)</span>\n\t<span class=\"token boolean\">_</span><span class=\"token punctuation\">,</span> err <span class=\"token operator\">:=</span> rand<span class=\"token punctuation\">.</span><span class=\"token function\">Read</span><span class=\"token punctuation\">(</span>buf<span class=\"token punctuation\">)</span>\n\t<span class=\"token keyword\">if</span> err <span class=\"token operator\">!=</span> <span class=\"token boolean\">nil</span> <span class=\"token punctuation\">{</span>\n\t\tlog<span class=\"token punctuation\">.</span><span class=\"token function\">Fatalf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"error while generating random string: %s\"</span><span class=\"token punctuation\">,</span> err<span class=\"token punctuation\">)</span>\n\t<span class=\"token punctuation\">}</span>\n\t<span class=\"token comment\">// fmt.Println(string(buf)) // not printable</span>\n\n\tprintable_password <span class=\"token operator\">:=</span> base64<span class=\"token punctuation\">.</span>StdEncoding<span class=\"token punctuation\">.</span><span class=\"token function\">EncodeToString</span><span class=\"token punctuation\">(</span>buf<span class=\"token punctuation\">)</span>\n\tfmt<span class=\"token punctuation\">.</span><span class=\"token function\">Println</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"generated password\"</span><span class=\"token punctuation\">,</span> printable_password<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">}</span>\n</code></pre>\n<h2 id=\"base64url\" tabindex=\"-1\">Base64Url <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h2>\n<ul>\n<li><a href=\"https://datatracker.ietf.org/doc/html/rfc4648#section-5\">RFC4648: Section 5: Base 64 Encoding with URL and Filename Safe Alphabet</a></li>\n<li><a href=\"https://hhow09.github.io/blog/charset-encoding/\">standard Base64</a> uses <code>+</code> and <code>/</code> for the last 2 characters, and <code>=</code> for padding.</li>\n<li>Base64Url uses <code>-</code> and <code>_</code> for the last 2 characters, and makes padding optional.</li>\n</ul>\n<h3 id=\"usage\" tabindex=\"-1\">Usage <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h3>\n<ul>\n<li>If the Base64-encoded text needs to be transmitted/saved where <code>+</code>, <code>/</code>, or <code>=</code> have special meaning, e.g. in URLs where all 3 does, then it is better to use <code>Base64Url</code>.</li>\n<li>If the Base64-encoded text needs to be transmitted/saved where <code>-</code> or <code>_</code> have special meaning, then it is better to use Standard Base64.</li>\n</ul>\n<h3 id=\"ref\" tabindex=\"-1\">Ref <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/charset-encoding/\">#</a></h3>\n<ul>\n<li><a href=\"https://youtu.be/ut74oHojxqo?si=XBBGiFS1kw4Q7Mj2\">Youtube:  Unicode, in friendly terms: ASCII, UTF-8, code points, character encodings, and more </a></li>\n<li><a href=\"https://bharatkalluri.com/posts/base64-size-increase-explanation\">Why is a base 64 encoded file 33% larger than the original?</a></li>\n<li><a href=\"https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\">The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)</a></li>\n</ul>\n",
			"date_published": "2023-11-12T00:00:00Z"
		}
		,
		{
			"id": "https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/",
			"url": "https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/",
			"title": "System Design Interview Vol.2 - Chapter 6. Ad Click Event Aggregation",
			"content_html": "<p>the content mainly comes from <a href=\"https://www.amazon.com/System-Design-Interview-Insiders-Guide/dp/1736049119\">System Design Interview Vol.2</a> - <a href=\"https://github.com/alex-xu-system/bytebytego/blob/main/system_design_links_vol2.md#chapter-6-ad-click-event-aggregation\">Chapter 6. Ad Click Event Aggregation</a></p>\n<p>design a ad click event aggregation system for near-realtime data.</p>\n<h2 id=\"tldr\" tabindex=\"-1\">TLDR <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h2>\n<ol>\n<li>data pipeline / streaming: message queue</li>\n<li>aggregation service: map reduce</li>\n<li>database: read-heavy, write-heavy, no update or transaction reqiured</li>\n<li>issues:\n<ul>\n<li>duplicate events</li>\n<li>exactly once processing</li>\n<li>fault tolerance</li>\n<li>hotspot</li>\n</ul>\n</li>\n<li>scaling\n<ul>\n<li>horizontal scaling in message queue, database and aggregaiton service</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"requirement\" tabindex=\"-1\">Requirement <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h2>\n<h3 id=\"functional-requirement\" tabindex=\"-1\">Functional Requirement <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<ol>\n<li>Support querying aggregated data: the number of clicks of certian ad (<code>ad_id</code>) in last <code>M</code> minutes</li>\n<li>Support querying aggregated data: top <code>N</code> most clicked ad in last <code>M</code> minutes.</li>\n<li>Support filtering by different attributes in above 2 querys.</li>\n<li>Dataset volume is FAANG scale\n<ul>\n<li>volume: single database is not a choice.</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"non-functional-requirement\" tabindex=\"-1\">Non-Functional Requirement <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<ol>\n<li>correctness of the aggregation result is important\n<ul>\n<li>as data is used for RTB and ads billing</li>\n</ul>\n</li>\n<li>Properly handle delayed duplicate events</li>\n<li>The system should be resilient to partial failures.</li>\n<li>End-to-end latency should be a few minutes, at most.\n<ul>\n<li>need realtime streaming system (instead of batch system)</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"estimation\" tabindex=\"-1\">Estimation <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<ul>\n<li>active users: 1e9 / day</li>\n<li>assume event per user: 1 / user</li>\n<li>QPS: <code>(1e9 * 1) / (1e5) = 1e5</code>\n<ul>\n<li>assume peak hour could be <code>5e5</code></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"high-level-design\" tabindex=\"-1\">High Level Design <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h2>\n<h3 id=\"data-model\" tabindex=\"-1\">Data Model <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<h4 id=\"raw-event\" tabindex=\"-1\">Raw Event <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<table>\n<thead>\n<tr>\n<th>ad_id</th>\n<th>timestamp</th>\n<th>user_id</th>\n<th>ip</th>\n<th>country_code</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ad001</td>\n<td>2023-06-01 00:00:01</td>\n<td>user1</td>\n<td>207.148.22.22</td>\n<td>US</td>\n</tr>\n<tr>\n<td>ad001</td>\n<td>2023-06-01 00:00:02</td>\n<td>user2</td>\n<td>209.153.55.11</td>\n<td>JP</td>\n</tr>\n<tr>\n<td>ad002</td>\n<td>2023-06-01 00:00:02</td>\n<td>user2</td>\n<td>209.153.55.11</td>\n<td>JP</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"aggregated-data\" tabindex=\"-1\">Aggregated Data <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<table>\n<thead>\n<tr>\n<th>ad_id</th>\n<th>timestamp_minute</th>\n<th>count</th>\n<th>filter_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ad001</td>\n<td>202306010000</td>\n<td>5</td>\n<td>0023</td>\n</tr>\n<tr>\n<td>ad001</td>\n<td>202306010000</td>\n<td>7</td>\n<td>0012</td>\n</tr>\n<tr>\n<td>ad002</td>\n<td>202306010001</td>\n<td>7</td>\n<td>0012</td>\n</tr>\n<tr>\n<td>ad002</td>\n<td>202306010001</td>\n<td>8</td>\n<td>0023</td>\n</tr>\n</tbody>\n</table>\n<p>data under same <code>ad_id-timestamp_minute</code> can be further group by <code>filter_id</code></p>\n<h3 id=\"query-api\" tabindex=\"-1\">Query API <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<h4 id=\"the-number-of-clicks-of-certian-ad-ad-id-in-last-m-minutes\" tabindex=\"-1\">the number of clicks of certian ad (<code>ad_id</code>) in last <code>M</code> minutes <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<ul>\n<li>API: <code>GET /v1/ads/{:ad_id}/aggregated_count</code></li>\n<li>query parameters:\n<ul>\n<li><code>from_minute</code></li>\n<li><code>to_minute</code></li>\n<li><code>filter_id</code></li>\n</ul>\n</li>\n<li>returns <code>ad_id</code> and <code>count</code></li>\n</ul>\n<h4 id=\"top-n-most-clicked-ad-in-last-m-minutes\" tabindex=\"-1\">top <code>N</code> most clicked ad in last <code>M</code> minutes <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<ul>\n<li>API: <code>GET /v1/ads/popular_ads</code></li>\n<li>query parameters:\n<ul>\n<li><code>count</code>: top <code>N</code></li>\n<li><code>window</code>: last <code>M</code> minutes</li>\n<li><code>filter_id</code></li>\n</ul>\n</li>\n<li>returns <code>ad_ids</code></li>\n</ul>\n<h3 id=\"data-storage\" tabindex=\"-1\">Data Storage <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<p>we need to store both raw data and aggregated data</p>\n<h4 id=\"comparison-between-querying-raw-data-vs-aggregated-data\" tabindex=\"-1\">Comparison between querying raw data vs aggregated data <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>query raw data</th>\n<th>query aggregated data</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>usage</td>\n<td>by other service</td>\n<td>this system</td>\n</tr>\n<tr>\n<td>pros</td>\n<td>full data set,  support ad-hoc filter</td>\n<td>smaller data set, fast query</td>\n</tr>\n<tr>\n<td>cons</td>\n<td>huge storage, slow query</td>\n<td>data loss</td>\n</tr>\n<tr>\n<td>where</td>\n<td>cold storage</td>\n<td>database</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"database-choice\" tabindex=\"-1\">Database Choice <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<h5>analyze</h5>\n<ul>\n<li>both read heavy and write-heavy\n<ul>\n<li>read: refresh the aggregation data in dashboard</li>\n<li>write: insert the data</li>\n</ul>\n</li>\n<li>no relation and transaction needed</li>\n<li>time series</li>\n</ul>\n<h5>Choices</h5>\n<ul>\n<li>InfluxDB <a href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">[1]</a></li>\n<li>Cassandra <a href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">[2]</a>\n<ul>\n<li>Netflix use cassandra for time series database <a href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">[3]</a></li>\n<li>storage engine: LSM tree <a href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">[4]</a> (good for write heavy)</li>\n</ul>\n</li>\n<li>S3 + <a href=\"https://www.databricks.com/glossary/what-is-parquet\">Parquet</a>\n<ul>\n<li>columnar storage</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"high-level-design-1\" tabindex=\"-1\">High Level Design <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/TjtMQlbZ7i-846.avif 846w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/TjtMQlbZ7i-846.webp 846w\"><img alt=\"Figure 3: High Level Design\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/TjtMQlbZ7i-846.svg\" width=\"846\" height=\"506\"></picture></p>\n<p>Kafka support high throughput, exact-once delivery.we can discuss exact-once delivery / atomic commit in <a href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">Delivery Guarantees</a></p>\n<h3 id=\"aggregation-service\" tabindex=\"-1\">Aggregation Service <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<h4 id=\"the-number-of-clicks-of-certian-ad-ad-id-in-last-m-minutes-1\" tabindex=\"-1\">the number of clicks of certian ad (<code>ad_id</code>) in last <code>M</code> minutes <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/z6RTVsMZhH-949.avif 949w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/z6RTVsMZhH-949.webp 949w\"><img alt=\"Figure 8 Aggregate the number of clicks\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/z6RTVsMZhH-949.svg\" width=\"949\" height=\"368\"></picture></p>\n<h2 id=\"deep-dive\" tabindex=\"-1\">Deep Dive <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h2>\n<h3 id=\"aggregation-window-and-watermark\" tabindex=\"-1\">Aggregation Window and Watermark <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<h4 id=\"what-is-watermarking\" tabindex=\"-1\">What Is Watermarking? <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<blockquote>\n<p>when working with real-time streaming data there will be delays between event time and processing time due to how data is ingested and whether the overall application experiences issues like downtime. Due to these potential variable delays, the engine that you use to process this data needs to have some mechanism to decide when to close the aggregate windows and produce the aggregate result.\n<a href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">[5]</a></p>\n</blockquote>\n<h3 id=\"delivery-guarantees\" tabindex=\"-1\">Delivery Guarantees <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<p>In most circumstances, <strong>at-least once</strong> processing is good enough if a small percentage of duplicates are acceptable. However, this is not the case for our system. Differences of a few percent in data points could result in discrepancies of millions of dollars.</p>\n<p>Therefore, we recommend <strong>exactly-once delivery</strong> for the system.</p>\n<h3 id=\"data-deduplication\" tabindex=\"-1\">Data deduplication <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<p>two common sources:</p>\n<ol>\n<li>client resend</li>\n<li>aggregation server outage</li>\n</ol>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/mPAtoBBGUB-1296.avif 1296w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/mPAtoBBGUB-1296.webp 1296w\"><img alt=\"Figure 17 Duplicate data\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/mPAtoBBGUB-1296.svg\" width=\"1296\" height=\"1005\"></picture></p>\n<p>If step 6 fails, perhaps due to Aggregator outage, events from 100 to 110 are already sent to the downstream, but the new offset 110 is not persisted in upstream Kafka. In this case, a new Aggregator would consume again from offset 100, even if those events are already processed, causing duplicate data.</p>\n<h4 id=\"solution\" tabindex=\"-1\">Solution <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/57_6r_SJe4-1791.avif 1791w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/57_6r_SJe4-1791.webp 1791w\"><img alt=\"Figure 20 Distributed transaction\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/57_6r_SJe4-1791.svg\" width=\"1791\" height=\"1233\"></picture></p>\n<p>To achieve <strong>exactly-once processing</strong> <a href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">[6]</a>, we need to put operations between step 4 to step 6 in one distributed transaction.</p>\n<p>Most common technique is two phase commit</p>\n<h2 id=\"scale-the-system\" tabindex=\"-1\">Scale the system <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h2>\n<h3 id=\"scale-the-message-queue\" tabindex=\"-1\">Scale the message queue <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<ol>\n<li>partition key: use <code>ad_id</code> as <strong>partition key</strong><a href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">[7]</a> so that an aggregation service can subscribe to all events of the same <code>ad_id</code>.</li>\n<li>number of partitions: if more consumers need to be added, try to do it during off-peak hours to minimize the impact.</li>\n<li>Topic physical sharding: We can split the data by geography (<code>topic_north_america</code>, <code>topic_europe</code>, <code>topic_asia</code>, etc.,) or by business type (<code>topic_web_ads</code>, <code>topic_mobile_ads</code>, etc).\n<ul>\n<li>Pros: increased system throughput, reduced rebalance time.</li>\n<li>Cons: extra complexity</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"scale-the-aggregation-service\" tabindex=\"-1\">Scale the aggregation service <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<ol>\n<li>multi-threading: Allocate events with different <code>ad_id</code>s to different threads.</li>\n</ol>\n<h3 id=\"scale-the-database\" tabindex=\"-1\">Scale the database <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<p>Cassandra natively supports horizontal scaling,</p>\n<h3 id=\"hotspot-issue\" tabindex=\"-1\">Hotspot issue <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<h4 id=\"aggregaion-service\" tabindex=\"-1\">Aggregaion Service <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<p>some <code>ad_id</code> might receive many more ad click events than others.</p>\n<p>Solution: dynamically allocate more node in aggregation service.</p>\n<h4 id=\"kafka\" tabindex=\"-1\">Kafka <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h4>\n<p>The publisher specifies the topic and the partition of a message before publishing. Hence, it’s the publisher’s responsibility to ensure that the partition logic will not result in a hot partition.</p>\n<p><a href=\"https://docs.confluent.io/platform/current/kafka/monitoring.html\">Confluent: Monitoring Kafka with JMX</a></p>\n<h3 id=\"fault-tolerance\" tabindex=\"-1\">Fault tolerance <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<ul>\n<li>aggregaion service works in memory and <strong>could fail</strong></li>\n<li>Replaying data from the beginning of Kafka is slow.</li>\n</ul>\n<p>Solution: snapshot aggregaion service to safe <strong>current state</strong> and <strong>event offset</strong></p>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/TFtUeSY7BI-558.avif 558w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/TFtUeSY7BI-558.webp 558w\"><img alt=\"Figure 27 Aggregation node failover\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/TFtUeSY7BI-558.svg\" width=\"558\" height=\"309\"></picture></p>\n<h2 id=\"data-monitoring-and-correctness\" tabindex=\"-1\">Data monitoring and correctness <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h2>\n<h3 id=\"continuous-monitoring\" tabindex=\"-1\">Continuous monitoring <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<ul>\n<li>Latency</li>\n<li>Message queue size\n<ul>\n<li>for aggregation servie to scale the node</li>\n<li>for application to apply back pressure.</li>\n</ul>\n</li>\n<li>System resources on aggregation nodes</li>\n</ul>\n<h3 id=\"reconciliation\" tabindex=\"-1\">Reconciliation <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h3>\n<ul>\n<li>purpose: comparing different sets of data in order to ensure data integrity.</li>\n<li>how: using a <strong>batch job</strong> and reconciling with the real-time aggregation result.</li>\n<li>why: some events might arrive late, the result from the batch job might not match exactly with the real-time aggregation result.</li>\n</ul>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/H38DsDbTtW-1033.avif 1033w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/H38DsDbTtW-1033.webp 1033w\"><img alt=\"Figure 28 Final design\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/H38DsDbTtW-1033.svg\" width=\"1033\" height=\"434\"></picture></p>\n<h2 id=\"reference\" tabindex=\"-1\">Reference <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/system-design-interview-add-click-aggregation/\">#</a></h2>\n<ol>\n<li><a href=\"https://www.influxdata.com/blog/influxdb-vs-cassandra-time-series/\">InfluxDB Tops Cassandra in Time Series Data &amp; Metrics Benchmark</a></li>\n<li><a href=\"https://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html\">Cassandra Time Series Data Modeling For Massive Scale</a></li>\n<li><a href=\"https://netflixtechblog.com/scaling-time-series-data-storage-part-i-ec2b6d44ba39\">Scaling Time Series Data Storage — Part I</a></li>\n<li><a href=\"https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml/dmlManageOndisk.html\">Apache Cassandra™ 3.x - Storage engine</a></li>\n<li><a href=\"https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html\">Feature Deep Dive: Watermarking in Apache Spark Structured Streaming</a></li>\n<li><a href=\"https://flink.apache.org/2018/02/28/an-overview-of-end-to-end-exactly-once-processing-in-apache-flink-with-apache-kafka-too/\">An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a></li>\n<li><a href=\"https://www.confluent.io/blog/kafka-streams-tables-part-2-topics-partitions-and-storage-fundamentals/#partition-events\">Streams and Tables in Apache Kafka: Topics, Partitions, and Storage Fundamentals</a></li>\n</ol>\n",
			"date_published": "2023-06-18T00:00:00Z"
		}
		,
		{
			"id": "https://hhow09.github.io/blog/paper-dremel/",
			"url": "https://hhow09.github.io/blog/paper-dremel/",
			"title": "Dremel: A Decade of Interactive SQL Analysis at Web Scale",
			"content_html": "<p>Paper: <a href=\"https://15721.courses.cs.cmu.edu/spring2023/papers/19-bigquery/p3461-melnik.pdf\">Dremel: A Decade of Interactive SQL Analysis at Web Scale</a></p>\n<h2 id=\"introduction\" tabindex=\"-1\">Introduction <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p>the main ideas we highlight in this paper are:</p>\n<ol>\n<li>SQL: simple SQL query to analyze nested data</li>\n<li>Disaggregated compute and storage: decouples compute from storage</li>\n<li>In situ analysis: different compute engines can operate on same piece of data</li>\n<li>Serverless computing: fully managed internal service</li>\n<li>Columnar storage</li>\n</ol>\n<h2 id=\"embracing-sql\" tabindex=\"-1\">Embracing SQL <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p>Because SQL doesn’t scale, GFS and MapReduce became the standard ways to store and process huge datasets. However writing analysis jobs on these systems is difficult and complex. Dremel was one of the first systems to <strong>reintroduce SQL for Big\nData analysis</strong>.</p>\n<p>In Google, nearly all data passed between applications or stored on disk was in <a href=\"https://protobuf.dev/\">Protocol Buffers</a>. Dremel's critical innovations includes <strong>first-class support for structured data</strong>. Dremel made it easy to query that hierarchical data with SQL.</p>\n<h2 id=\"disaggregated-storage\" tabindex=\"-1\">Disaggregated storage <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p>Dremel started with shared-nothing servers and computation is coupled with storage.</p>\n<p>In 2009, Dremel was migrated to <a href=\"https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood\">Borg (cluster management)</a> and replicated storage. It accommodate the growing query workload and improve the utilization of the service. However <strong>storage and processing are still coupled</strong>. Which means</p>\n<ol>\n<li>all algorithms needed to be replication-aware</li>\n<li>resizing server also need to <strong>move data</strong> around.</li>\n<li>Resources can only be accessed by Dremel.</li>\n</ol>\n<p>Later Dremel was migrated from local disk to GFS. Performance was initially degraded. After lots of fine-tuning,  disaggregated storage outperformed the local-disk based system. There are several advantages:</p>\n<ol>\n<li>improved SLOs and robustness of Dremel because GFS is fully-managed service.</li>\n<li>No more need to load from GFS onto Dremel server’s local disks.</li>\n<li>No need to resize our clusters in order to load new data.</li>\n</ol>\n<p>Another notch of scalability and robustness was gained once Google’s file system was migrated from GFS (single-master model) to <a href=\"https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood\">Colossus (distributed multi-master model)</a>.</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/ZNSywBCsI0-990.avif 990w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/ZNSywBCsI0-990.webp 990w\"><img alt=\"Figure 1: Disaggregated storage, memory, and compute\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/ZNSywBCsI0-990.png\" width=\"990\" height=\"512\"></picture></p>\n<h2 id=\"disaggregated-memory\" tabindex=\"-1\">Disaggregated memory <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p>Dremel added support for <strong>distributed joins</strong> with <strong>shuffle</strong> utilizing <strong>local RAM and disk</strong> to store sorted intermediate results. However there is bottleneck in scalability and multi-tenancy due to the  coupling between the compute nodes intermediate shuffle storage.</p>\n<p>In 2014, Dremel shuffle migrated to a <a href=\"https://cloud.google.com/blog/products/bigquery/in-memory-query-execution-in-google-bigquery\">new shuffle infrastructure</a>. Shuffle data were stored in a distributed transient storage system. Improved peformance in-terms of latency and larger shuffle and service cost was observed.</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/7rg3rOR0mF-1164.avif 1164w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/7rg3rOR0mF-1164.webp 1164w\"><img alt=\"Disaggregated in-memory shuffle\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/7rg3rOR0mF-1164.png\" width=\"1164\" height=\"1012\"></picture></p>\n<h2 id=\"observations\" tabindex=\"-1\">Observations <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p>Disaggregation proved to be a major trend in data management.</p>\n<ol>\n<li>Economies of scale</li>\n<li>Universality</li>\n<li>Higher-level APIs:  Storage access is far removed from the early block I/O APIs.</li>\n<li>Value-added repackaging</li>\n</ol>\n<h2 id=\"in-situ-data-analysis\" tabindex=\"-1\">In Situ Data Analysis <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p>The data management community finds itself today in the middle of a transition from classical data warehouses to a <strong>datalake-oriented architecture</strong> for analytics. The trend includes:</p>\n<ol>\n<li>consuming data from a variety of data sources</li>\n<li>eliminating traditional ETL-based data ingestion from an OLTP system to a data warehouse</li>\n<li>enabling a variety of compute engines to operate on the data.</li>\n</ol>\n<h3 id=\"dremel-s-evolution-to-in-situ-analysis\" tabindex=\"-1\">Dremel’s evolution to in situ analysis <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h3>\n<p>Dremel’s initial design in 2006 was reminiscent of traditional DBMSs:</p>\n<ol>\n<li>explicit data loading was required.</li>\n<li>the data was stored in a proprietary format.</li>\n<li>inaccessible to other tools.</li>\n</ol>\n<p>As part of migrating Dremel to GFS, Dremel open the <strong>storage format</strong> (as a library) within Google which has 2 distinguising properties: <strong>columnar</strong>, <strong>self-dsecribing</strong>. The <strong>self-dsecribing</strong> feature enables interoperation between custom data transformation tools and SQL-based analytics.  MapReduce jobs could run on columnar data, write out columnar results, and those results could be immediately queried via Dremel. Users no longer had to load data into their data warehouse, any file they had in the GFS could effectively be queryable.</p>\n<p>In situ approach was evolved in two complementary directions:</p>\n<ol>\n<li>adding file formats beyond original columnar format.</li>\n<li>expanding the universe of joinable data ( e.g. BigTable, Cloud Storage, Google Drive ).</li>\n</ol>\n<h3 id=\"drawbacks-of-in-situ-analysis\" tabindex=\"-1\">Drawbacks of in situ analysis <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h3>\n<ol>\n<li>users do not always want to or have the capability to manage their own data safely and securely</li>\n<li>in situ analysis means there is no opportunity to either optimize storage layout or compute statistics in the general case.</li>\n</ol>\n<h2 id=\"serverless-computing\" tabindex=\"-1\">Serverless Computing <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<h3 id=\"serverless-roots\" tabindex=\"-1\">Serverless roots <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h3>\n<ul>\n<li>Most traditional DBMS and data-warehouse were deployed on dedicated servers.</li>\n<li>MapReduce and Hadoop uses virtual machines but are still single-tenant.</li>\n</ul>\n<p>Three core ideas in Dremel which enable serverless analytics:</p>\n<ol>\n<li>Disaggregation of compute, storage and memory</li>\n<li>Fault Tolerance and Restartability\n<ul>\n<li>each subtask are deterministic and repeatable</li>\n<li>task dispatcher may need to dispatching multiple copies of the same task to alleviate unresponsive workers.</li>\n</ul>\n</li>\n<li>Virtual Scheduling Units\n<ul>\n<li>Dremel scheduling logic was designed to work with abstract units of compute and memory\ncalled slots</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"evolution-of-serverless-architecture\" tabindex=\"-1\">Evolution of serverless architecture <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h3>\n<p>Dremel continued to evolve its serverless capabilities, making them one of the key characteristics of Google BigQuery today.</p>\n<ol>\n<li>\n<p>Centralized Scheduling</p>\n<ul>\n<li>The scheduler uses the entire cluster state to make scheduling decisions which enables better utilization and isolation. (Figure 3)</li>\n</ul>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/JY2-2bu-Gn-917.avif 917w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/JY2-2bu-Gn-917.webp 917w\"><img alt=\"System architecture and execution inside a server\nnode\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/JY2-2bu-Gn-917.png\" width=\"917\" height=\"636\"></picture></p>\n</li>\n<li>\n<p>Shuffle Persistence Layer</p>\n<ul>\n<li>allow decoupling scheduling and execution of different stages of the query.</li>\n</ul>\n</li>\n<li>\n<p>Flexible Execution DAGs</p>\n<ul>\n<li>query coordinator builds the query plan (tree) and send to the workers</li>\n<li>Workers from the leaf stage read from the storage layer and write to the shuffle persistence layer.</li>\n</ul>\n<p><picture><source type=\"image/avif\" srcset=\"https://hhow09.github.io/img/t7gCl1YkGY-708.avif 708w\"><source type=\"image/webp\" srcset=\"https://hhow09.github.io/img/t7gCl1YkGY-708.webp 708w\"><img alt=\"Shuffle-based execution plan\" loading=\"lazy\" decoding=\"async\" src=\"https://hhow09.github.io/img/t7gCl1YkGY-708.png\" width=\"708\" height=\"526\"></picture></p>\n</li>\n<li>\n<p>Dynamic Query Execution</p>\n<ul>\n<li>query execution plan can dynamically change during runtime based on the statistics collected during query execution.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"columar-storage-for-nested-data\" tabindex=\"-1\">Columar Storage For Nested Data <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p>The main design decision behind <strong>repetition and definition levels encoding</strong> was to encode all structure information within the column itself, so it can be accessed without reading ancestor fields.</p>\n<ul>\n<li>repetition level: specifies for repeated values whether each ancestor record is appended into or starts a new value</li>\n<li>definition level: specifies which ancestor records are absent when an optional field is absent.</li>\n</ul>\n<p>In 2014, migration of the storage to an improved columnar format, <a href=\"https://cloud.google.com/blog/products/bigquery/inside-capacitor-bigquerys-next-generation-columnar-storage-format\">Capacitor</a>.</p>\n<h2 id=\"embedded-evaluation\" tabindex=\"-1\">Embedded evaluation <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p><strong>Capacitor</strong> uses a number of techniques to make filtering efficient</p>\n<ol>\n<li>Partition and predicate pruning\n<ul>\n<li>Various statistics are maintained about the values in each column. They are used both to eliminate partitions that are guaranteed to not contain any matching rows, and to simplify the filter by removing tautologies</li>\n</ul>\n</li>\n<li>Vectorization: <a href=\"https://15721.courses.cs.cmu.edu/spring2019/papers/10-compression/abadi-sigmod2006.pdf\">Bit-Vector Encoding</a></li>\n<li>Skip-indexes\n<ul>\n<li>Capacitor combines column values into segments, which are compressed individually.</li>\n<li>The column header contains an index with offsets pointing to the beginning of each segment.</li>\n<li>When the filter is very selective, Capacitor uses this index to skip segments that have no hits, avoiding their decompression.</li>\n</ul>\n</li>\n<li>Predicate reordering\n<ul>\n<li>Capacitor uses a number of heuristics to make filter reordering decisions, which take into account dictionary usage, unique value cardinality,</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"row-reordering\" tabindex=\"-1\">Row reordering <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<ul>\n<li>run-length encodings (RLE) in particular is very sensitive to <strong>row ordering</strong>.</li>\n<li>Usually, row order in the table does not have significance, so Capacitor is free to permute rows to improve RLE effectiveness.</li>\n<li>Capacitor’s row reordering algorithm uses sampling and heuristics to build an approximate model.</li>\n</ul>\n<h2 id=\"interactive-query-latency-over-big-data\" tabindex=\"-1\">Interactive Query Latency Over Big Data <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<p>latency-reducing techniques implemented in Dremel:</p>\n<ol>\n<li>\n<p>Stand-by server pool</p>\n</li>\n<li>\n<p>Speculative execution</p>\n<ul>\n<li>small, fine-grained task</li>\n<li>duplicate tasks to prevent slow worker</li>\n</ul>\n</li>\n<li>\n<p>Multi-level execution trees</p>\n</li>\n<li>\n<p>Column-oriented schema representation</p>\n</li>\n<li>\n<p>Balancing CPU and IO with lightweight compression</p>\n</li>\n<li>\n<p>Approximate results</p>\n<ul>\n<li>Dremel uses one-pass algorithms that work well with the multi-level execution tree architecture.</li>\n</ul>\n</li>\n<li>\n<p>Query latency tier</p>\n<ul>\n<li>The dispatcher needed to be able to preempt the processing of parts of a query to allow a new user’s query to be processed.</li>\n</ul>\n</li>\n<li>\n<p>Reuse of file operations</p>\n<ul>\n<li>bottleneck for achieving low latency as thousands of Dremel workers send requests to the file system master(s) for metadata and to the chunk servers for open and read operations.</li>\n<li>The most important one: reuse metadata obtained from the file system by fetching it in a batch at the root server and passing it through the execution tree to the leaf servers for data reads.</li>\n</ul>\n</li>\n<li>\n<p>Guaranteed capacity</p>\n<ul>\n<li>a customer could reserve some capacity and use that capacity only for latency-sensitive workloads.</li>\n</ul>\n</li>\n<li>\n<p>Adaptive query scaling</p>\n</li>\n</ol>\n<h2 id=\"reference\" tabindex=\"-1\">Reference <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/paper-dremel/\">#</a></h2>\n<ul>\n<li><a href=\"https://15721.courses.cs.cmu.edu/spring2023/papers/19-bigquery/p3461-melnik.pdf\">Dremel: A Decade of Interactive SQL Analysis at Web Scale</a></li>\n<li><a href=\"https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood\">BigQuery under the hood</a></li>\n<li><a href=\"https://cloud.google.com/blog/products/bigquery/in-memory-query-execution-in-google-bigquery\">In-memory query execution in Google BigQuery</a></li>\n<li><a href=\"https://cloud.google.com/blog/products/bigquery/inside-capacitor-bigquerys-next-generation-columnar-storage-format\">Inside Capacitor, BigQuery’s next-generation columnar storage format</a></li>\n</ul>\n",
			"date_published": "2023-06-10T00:00:00Z"
		}
		,
		{
			"id": "https://hhow09.github.io/blog/distributed-id-generation/",
			"url": "https://hhow09.github.io/blog/distributed-id-generation/",
			"title": "Distributed ID Generation Solutions",
			"content_html": "<p>Using auto-increment primary keys from traditional databases as ID for distributed systems can be inefficient and vulnerable to being predicted and analyzed.</p>\n<p>Nowadays, distributed systems require unique global identifiers; it is an essential task in distributed computing with growing internet usage.</p>\n<h2 id=\"requirement\" tabindex=\"-1\">Requirement <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h2>\n<h3 id=\"functional-requirement\" tabindex=\"-1\">Functional Requirement <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h3>\n<ul>\n<li>id should be globally unique</li>\n<li>low latency</li>\n<li>each node can generate id independently</li>\n</ul>\n<h3 id=\"nice-to-have-features\" tabindex=\"-1\">Nice to have features <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h3>\n<ul>\n<li>monotonicity (sortable by time)</li>\n<li>unpredictable</li>\n<li>high availability. Since an ID generator is a mission-critical system, it must be highly available.</li>\n</ul>\n<h2 id=\"popular-solutions\" tabindex=\"-1\">Popular Solutions <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h2>\n<ol>\n<li>UUID V4</li>\n<li>Nano ID</li>\n<li>ULID</li>\n<li>KSUID</li>\n<li>Mongdb objectID</li>\n<li>Snowflake ID</li>\n</ol>\n<h2 id=\"overview\" tabindex=\"-1\">Overview <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h2>\n<p>Designing a distributed ID generation scheme can be divided into two steps</p>\n<ol>\n<li>Generate the binary ID, usually selected from pseudo-random numbers, time, and node ID.</li>\n<li>Convert the binary ID into a human-readable and easily transmittable string text.</li>\n</ol>\n<h2 id=\"binary-id-generation\" tabindex=\"-1\">Binary ID Generation <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h2>\n<h3 id=\"1-prng-pseudo-random-number-generator\" tabindex=\"-1\">1. PRNG (pseudo random number generator) <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h3>\n<ol>\n<li>UUID v4 (128 bit)</li>\n<li>Nano ID (no standard length)</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th>random number (bits)</th>\n<th>total (bits)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">UUID v4</td>\n<td>122</td>\n<td>128</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Nano ID</td>\n<td>126</td>\n<td>126</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"uuid-2\" tabindex=\"-1\">UUID <a href=\"https://hhow09.github.io/blog/distributed-id-generation/\">[2]</a> <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<blockquote>\n<p>UUIDv4 consists of 128 bits, which are typically represented as 32 hexadecimal characters in the following format: xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx, where the digit &quot;4&quot; in the third group signifies the version (i.e., UUIDv4) and the digit &quot;y&quot; in the fourth group specifies the variant.</p>\n</blockquote>\n<h4 id=\"pros\" tabindex=\"-1\">Pros <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<ul>\n<li>Generating UUID is simple. No coordination between servers is needed so there will not be any synchronization issues.</li>\n<li>The system is easy to scale because each web server is responsible for generating IDs they consume. ID generator can easily scale with web servers.</li>\n</ul>\n<h4 id=\"cons\" tabindex=\"-1\">Cons <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<ul>\n<li>IDs are 128 bits long, but our requirement is 64 bits.</li>\n<li>IDs do not go up with time.</li>\n<li>IDs could be non-numeric.</li>\n</ul>\n<h5>Chance of duplicate ID</h5>\n<blockquote>\n<p>With the sheer number of possible combinations (2^128), it would be almost impossible to generate a duplicate unless you are generating trillions of IDs every second, for many years.</p>\n</blockquote>\n<h4 id=\"nano-id-3\" tabindex=\"-1\">Nano ID <a href=\"https://hhow09.github.io/blog/distributed-id-generation/\">[3]</a> <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<blockquote>\n<p>A tiny, secure, URL-friendly, unique string ID generator.\nSmall. 130 bytes (minified and gzipped). No dependencies. Size Limit controls the size.\nSafe. It uses hardware random generator. Can be used in clusters.\nShort IDs. It uses a larger alphabet than UUID (A-Za-z0-9_-). So ID size was reduced from 36 to 21 symbols.\nPortable. Nano ID was ported to 20 programming languages.</p>\n</blockquote>\n<h3 id=\"2-timestamp-prng\" tabindex=\"-1\">2. Timestamp + PRNG <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th>timestamp (bits)</th>\n<th>random number (bits)</th>\n<th>total (bits)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">ULID</td>\n<td>48</td>\n<td>80</td>\n<td>128</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">KSUID</td>\n<td>32</td>\n<td>128</td>\n<td>160</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"ulid-4\" tabindex=\"-1\">ULID <a href=\"https://hhow09.github.io/blog/distributed-id-generation/\">[4]</a> <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<ul>\n<li>128-bit compatibility with UUID</li>\n<li>Lexicographically sortable!\n<ul>\n<li>timestamp: UNIX-time in milliseconds</li>\n</ul>\n</li>\n<li>No special characters (URL safe)</li>\n<li>Monotonic sort order (correctly detects and handles the same millisecond)</li>\n</ul>\n<h4 id=\"ksuid-4\" tabindex=\"-1\">KSUID <a href=\"https://hhow09.github.io/blog/distributed-id-generation/\">[4]</a> <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<blockquote>\n<p>There are numerous methods for generating unique identifiers, so why KSUID?</p>\n<ol>\n<li>Naturally ordered by generation time</li>\n<li>Collision-free, coordination-free, dependency-free</li>\n<li>Highly portable representations</li>\n</ol>\n</blockquote>\n<h3 id=\"3-timestamp-node-id-monotonic-counter\" tabindex=\"-1\">3. Timestamp + Node ID + Monotonic Counter <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th>timestamp (bits)</th>\n<th>Node Id (bits)</th>\n<th>Process Id (bits)</th>\n<th>Counter (bits)</th>\n<th>total (bits)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Snowflake ID</td>\n<td>41</td>\n<td>10</td>\n<td>-</td>\n<td>12</td>\n<td>64</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MongoDB ObjectID</td>\n<td>32</td>\n<td>24</td>\n<td>16</td>\n<td>24</td>\n<td>96</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>id is unique across different node and process</li>\n<li>counter ensure the uniqueness on same process of same node</li>\n</ul>\n<h4 id=\"security-concern\" tabindex=\"-1\">Security Concern <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<ul>\n<li>since the structure of id, the entropy of id is quite small, which means <a href=\"https://github.com/andresriancho/mongo-objectid-predict\">easy to predict</a>.</li>\n<li>It is dangerous through <a href=\"https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/05-Authorization_Testing/04-Testing_for_Insecure_Direct_Object_References\">IDOR</a></li>\n</ul>\n<h4 id=\"snowflake-id-6\" tabindex=\"-1\">Snowflake ID <a href=\"https://hhow09.github.io/blog/distributed-id-generation/\">[6]</a> <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<ul>\n<li>timestamp: Epoch in milliseconds precision\n<ul>\n<li>That gives us <strong>69</strong> years with respect to a <strong>custom epoch</strong>.</li>\n</ul>\n</li>\n<li>Node ID: 10 bits, this gives us 1024 nodes/machines.</li>\n<li>Local counter: 12 bits, The counter’s max value would be 4095.</li>\n</ul>\n<h4 id=\"mongodb-objectid-7\" tabindex=\"-1\">MongoDB ObjectID <a href=\"https://hhow09.github.io/blog/distributed-id-generation/\">[7]</a> <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<ul>\n<li>\n<p>The object ID is generated by the MongoDB driver instead of the database.</p>\n</li>\n<li>\n<p>The 12-byte ObjectId consists of:</p>\n<ul>\n<li>A 4-byte timestamp, representing the ObjectId's creation, measured in seconds since the Unix epoch.</li>\n<li>A 5-byte random value generated once per process. This random value is unique to the machine and process.</li>\n<li>A 3-byte incrementing counter, initialized to a random value.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"types-of-data-source\" tabindex=\"-1\">Types of data source <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h2>\n<h3 id=\"pseudo-random-number\" tabindex=\"-1\">Pseudo random number <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h3>\n<h4 id=\"packages\" tabindex=\"-1\">Packages <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<ul>\n<li><a href=\"https://pkg.go.dev/crypto/rand\">crypto/rand</a></li>\n<li><a href=\"https://pkg.go.dev/math/rand\">math/rand</a></li>\n</ul>\n<h4 id=\"considerations\" tabindex=\"-1\">Considerations <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h4>\n<ul>\n<li>collisoin: longer bits gives lower chance of collision</li>\n<li>unpredictability: use <code>crypto/rand</code> to lower unpredictability</li>\n</ul>\n<h2 id=\"reference\" tabindex=\"-1\">Reference <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-id-generation/\">#</a></h2>\n<ol>\n<li><a href=\"https://catcat.cc/post/2020-09-19/\">6 个流行的分布式 ID 方案之间的对决</a></li>\n<li><a href=\"https://datatracker.ietf.org/doc/html/rfc4122\">UUID</a></li>\n<li><a href=\"https://github.com/ai/nanoid\">Nano ID</a></li>\n<li><a href=\"https://github.com/ulid/spec\">ULID</a></li>\n<li><a href=\"https://github.com/segmentio/ksuid\">KSUID</a></li>\n<li><a href=\"https://github.com/bwmarrin/snowflake\">Snowflake ID</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/reference/method/ObjectId/\">MongoDB ObjectID</a></li>\n</ol>\n",
			"date_published": "2023-05-02T00:00:00Z"
		}
		,
		{
			"id": "https://hhow09.github.io/blog/distributed-system-index/",
			"url": "https://hhow09.github.io/blog/distributed-system-index/",
			"title": "Distributed System Index",
			"content_html": "<h2 id=\"course-books\" tabindex=\"-1\">Course / Books <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h2>\n<ul>\n<li><a href=\"https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/\">Designing Data Intensive Applications</a></li>\n<li><a href=\"https://pdos.csail.mit.edu/6.824/index.html\">MIT 6.824: Distributed Systems</a></li>\n<li><a href=\"https://www.cl.cam.ac.uk/teaching/2122/ConcDisSys/\">CAM: Concurrent and Distributed Systems</a></li>\n<li><a href=\"https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB\">Distributed Systems lecture series: Martin Kleppmann</a>\n<ul>\n<li><a href=\"https://www.cl.cam.ac.uk/teaching/2122/ConcDisSys/dist-sys-notes.pdf\">course notes</a></li>\n</ul>\n</li>\n<li><a href=\"https://martinfowler.com/articles/patterns-of-distributed-systems/\">Martin Fowler: Patterns of Distributed Systems</a></li>\n</ul>\n<h2 id=\"cap-theorem\" tabindex=\"-1\">CAP Theorem <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h2>\n<ul>\n<li><a href=\"https://github.com/henryr/cap-faq\">henryr/cap-faq</a></li>\n</ul>\n<h3 id=\"issues\" tabindex=\"-1\">Issues <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h3>\n<ul>\n<li>split brain</li>\n</ul>\n<h2 id=\"consistency-models\" tabindex=\"-1\">Consistency Models <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h2>\n<p>overview: <a href=\"https://jepsen.io/consistency\">jepsen/consistency</a></p>\n<h3 id=\"linearizability\" tabindex=\"-1\">linearizability <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h3>\n<h3 id=\"causal-consistency\" tabindex=\"-1\">causal consistency <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h3>\n<ul>\n<li><a href=\"https://www.exhypothesi.com/clocks-and-causality/\">Clocks and Causality - Ordering Events in Distributed Systems</a></li>\n<li><a href=\"https://jepsen.io/consistency/models/causal\">Jepsen: Causal Consistency</a></li>\n<li><a href=\"https://vkontech.com/causal-consistency-guarantees-case-studies/#What_is_Causal_Consistency\">Causal Consistency Guarantees – Case Studies</a></li>\n<li><a href=\"https://timilearning.com/posts/mit-6.824/lecture-17-cops/\">MIT 6.824: Lecture 17 - Causal Consistency, COPS</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/\">Mongodb: Causal Consistency and Read and Write Concerns</a></li>\n</ul>\n<h3 id=\"eventual-consistency\" tabindex=\"-1\">eventual consistency <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h3>\n<h2 id=\"replication\" tabindex=\"-1\">Replication <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h2>\n<ul>\n<li><code>Designing Data Intensive Applications Chapter 5 Replication</code></li>\n</ul>\n<h3 id=\"models-of-replication\" tabindex=\"-1\">Models of Replication <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h3>\n<ul>\n<li>Leaders and Followers</li>\n<li>Multi-Leader Replication</li>\n<li>Leaderless Replication</li>\n</ul>\n<h3 id=\"write-ahead-log\" tabindex=\"-1\">Write Ahead Log <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h3>\n<ul>\n<li><a href=\"https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html\">Martin Fowler: Write-Ahead Log</a></li>\n<li><a href=\"https://www.oreilly.com/library/view/distributed-services-with/9781680508376/f_0025.xhtml\">Distributed Services with Go: Chapter 3 Write a Log Package</a></li>\n<li><a href=\"https://www.postgresql.org/docs/current/wal-intro.html\">PostGreSQL: 30.3. Write-Ahead Logging (WAL)</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/core/replica-set-oplog/\">MongoDB Oplog</a></li>\n</ul>\n<h3 id=\"postgresql\" tabindex=\"-1\">PostGreSQL <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h3>\n<ul>\n<li>physical v.s. Logical streaming replication</li>\n</ul>\n<h3 id=\"mongodb\" tabindex=\"-1\">MongoDB <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h3>\n<ul>\n<li><a href=\"https://www.mongodb.com/docs/manual/replication/\">MongoDB Manual: Replication</a></li>\n</ul>\n<h2 id=\"partition\" tabindex=\"-1\">Partition <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h2>\n<h2 id=\"raft-consensus-algorithm\" tabindex=\"-1\">Raft (Consensus Algorithm) <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h2>\n<ul>\n<li><a href=\"https://raft.github.io/\">The Raft Consensus Algorithm</a></li>\n<li><a href=\"http://thesecretlivesofdata.com/raft/\">The Secret Lives of Data: Raft</a></li>\n<li><a href=\"https://youtu.be/uXEYuDwm7e4\">Distributed Systems 6.2: Raft</a></li>\n<li><a href=\"https://www.cl.cam.ac.uk/teaching/2122/ConcDisSys/dist-sys-notes.pdf\">University of Cambridge: Distributed Systems</a></li>\n</ul>\n<h2 id=\"rum-conjecture\" tabindex=\"-1\">RUM Conjecture <a class=\"header-anchor\" href=\"https://hhow09.github.io/blog/distributed-system-index/\">#</a></h2>\n<blockquote>\n<p>The Trade offs Behind Modern Storage Systems</p>\n</blockquote>\n<ul>\n<li><a href=\"http://daslab.seas.harvard.edu/rum-conjecture/\">The RUM Conjecture</a></li>\n<li><a href=\"https://edward-huang.com/distributed-system/2021/01/24/the-trade-offs-behind-modern-storage-systems/\">The Trade offs Behind Modern Storage Systems</a></li>\n<li><a href=\"https://youtu.be/wxcCHvQeZ-U\">Youtube: Algorithms behind Modern Storage Systems</a></li>\n</ul>\n",
			"date_published": "2023-04-23T00:00:00Z"
		}
		
	]
}
